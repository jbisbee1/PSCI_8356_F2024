<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>The Big Matrix OLS Jam</title>
    <meta charset="utf-8" />
    <meta name="author" content="Prof. Bisbee" />
    <script src="libs/header-attrs-2.18/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="css/lexis.css" type="text/css" />
    <link rel="stylesheet" href="css/lexis-fonts.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# The Big Matrix OLS Jam
]
.subtitle[
## *Please email typos / corrections to <code>james.h.bisbee@vanderbilt.edu</code>
]
.author[
### Prof. Bisbee
]
.institute[
### Vanderbilt University
]
.date[
### Lecture Date: 2023/11/09
Slides Updated: 2023-12-23
]

---


&lt;style type="text/css"&gt;
.small .remark-code { /*Change made here*/
  font-size: 85% !important;
}
.tiny .remark-code { /*Change made here*/
  font-size: 50% !important;
}
&lt;/style&gt;



# Agenda

1. Matrix Algebra fun!

2. Multiple Regression

3. Controls

---

# Matrix Algebra Fun! [Thanks BK!](https://bkenkel.com/pdaps/matrix.html#vector-operations)

- Vectors: ordered arrays denoted `\(\mathbf{v} = (v_1,v_2,\dots,v_k)\)` or 

$$
`\begin{aligned}
   \mathbf{v}=
  \left( {\begin{array}{c}
   v_1 \\
   v_2 \\
   \vdots\\
   v_k
  \end{array} } \right)
\end{aligned}`
$$

  - (Note that some will denote vectors with bold letters, or with `\(\vec{v}\)`)

--

- Addition and subtraction require two vectors of the same length, `\(\mathbf{u}\)` and `\(\mathbf{v}\)`, but are then just adding or subtracting the elements

$$
`\begin{aligned}
   \mathbf{u} \pm \mathbf{v}=
  \left( {\begin{array}{c}
   u_1 \pm v_1 \\
   u_2 \pm v_2 \\
   \vdots\\
   u_k \pm v_k
  \end{array} } \right)
\end{aligned}`
$$

---

# Vectors

- Multiplication by a constant `\(c\)` is just multiplying each element by `\(c\)`

$$
`\begin{aligned}
   c\mathbf{v}=
  \left( {\begin{array}{c}
   cv_1 \\
   cv_2 \\
   \vdots\\
   cv_k
  \end{array} } \right)
\end{aligned}`
$$

--

- Multiplication of two vectors is called a **dot product**, written `\(\mathbf{u} \cdot \mathbf{v}\)`, and translates to multiplying each element in `\(\mathbf{u}\)` by the corresponding element in `\(\mathbf{v}\)` and then adding them all up

$$
`\begin{aligned}
\mathbf{u} \cdot \mathbf{v} &amp;= u_1v_1 + u_2v_2 + \dots + u_kv_k\\
&amp;= \sum_{m=1}^k u_mv_m
\end{aligned}`
$$

---

# Matrices

- A matrix is a two-dimensional array with entries in `\(n\)` rows and `\(m\)` columns, called an `\(n\times m\)` matrix

$$
`\begin{aligned}
   \mathbf{A}=
  \left[ {\begin{array}{ccc}
   a_{11} &amp; a_{12} &amp; a_{13} \\
   a_{21} &amp; a_{22} &amp; a_{23} \\
   a_{31} &amp; a_{32} &amp; a_{33}
  \end{array} } \right]
\end{aligned}`
$$

--

- As with vectors, matrices can be added and subtracted *as long as they are the same dimensions*

$$
`\begin{aligned}
   \mathbf{A} \pm \mathbf{B}=
  \left[ {\begin{array}{ccc}
   a_{11} \pm b_{11} &amp; a_{12} \pm b_{12} &amp; a_{13} \pm b_{13} \\
   a_{21} \pm b_{21} &amp; a_{22} \pm b_{22} &amp; a_{23} \pm b_{23} \\
   a_{31} \pm b_{31} &amp; a_{32} \pm b_{32} &amp; a_{33} \pm b_{33}
  \end{array} } \right]
\end{aligned}`
$$

--

- As with vectors, matrices multiplied by a constant are straightforward

$$
`\begin{aligned}
   c\mathbf{A}=
  \left[ {\begin{array}{ccc}
   ca_{11} &amp; ca_{12} &amp; ca_{13} \\
   ca_{21} &amp; ca_{22} &amp; ca_{23} \\
   ca_{31} &amp; ca_{32} &amp; ca_{33}
  \end{array} } \right]
\end{aligned}`
$$

---

# Matrices: Transpose

- Transposing: we can "rotate" `\(n\times m\)` matrices into `\(m\times n\)` matrices

  - Meaning that the first row becomes the first column, the second row becomes the second column, etc.
  
  - Denoted with `\(\mathbf{A}^\top\)` (or sometimes `\(\mathbf{A}'\)`)

--

- For example:

$$
`\begin{aligned}
\mathbf{A}=
\left[ {\begin{array}{ccc}
   99 &amp; 73 &amp; 2 \\
   13 &amp; 40 &amp; 41
  \end{array} } \right]
  \qquad
  \Leftrightarrow
  \qquad
\mathbf{A}^\top=
\left[ {\begin{array}{cc}
   99 &amp; 13 \\
   73 &amp; 40 \\
   2 &amp; 41
  \end{array} } \right]
\end{aligned}`
$$

---

# Matrices: Transpose

- Properties of transposes

$$
`\begin{aligned}
(\mathbf{A}^\top)^\top &amp;= \mathbf{A}, \\
(c \mathbf{A})^\top &amp;= c (\mathbf{A}^\top), \\
(\mathbf{A} + \mathbf{B})^\top &amp;= \mathbf{A}^\top + \mathbf{B}^\top, \\
(\mathbf{A} - \mathbf{B})^\top &amp;= \mathbf{A}^\top - \mathbf{B}^\top,\\
(\mathbf{A}\mathbf{B})^\top &amp;= \mathbf{B}^\top\mathbf{A}^\top.
\end{aligned}`
$$

--

- Note that it doesn't make sense to transpose a scalar

  - But also that this means a scalar is always equal to its transpose: `\(a = a^\top\)`

  
  

---

# Matrix Multiplication

- Refresher: need to multiply an `\(n \times m\)` matrix by an `\(m \times p\)` matrix.

  - **NOTE**: the number of rows in the second matrix must be equal to the number of columns in the first matrix!
  
- Resulting matrix is an `\(n \times p\)` matrix whose `\(ij\)`'th element is the **dot product** of the `\(i\)`'th row of the first matrix and the `\(j\)`'th column of the second matrix

--

- Try it: solve `\(\mathbf{AB}\)` where

$$
`\begin{aligned}
\mathbf{A} =\left[ {\begin{array}{ccc}
   2 &amp; 10 \\
   0 &amp; 1 \\
   -1 &amp; 5
  \end{array} } \right]
  \qquad 
  \text{ and }
  \qquad
\mathbf{B} = 
\left[ {\begin{array}{cc}
   1 &amp; 4 \\
   -1 &amp; 10
  \end{array} } \right]
\end{aligned}`
$$
--

$$
`\begin{aligned}
\mathbf{A} \mathbf{B} = 
\left[ {\begin{array}{cc}
   2 \cdot 1 + 10 \cdot (-1) &amp; 2 \cdot 4 + 10 \cdot 10 \\
  0 \cdot 1 + 1 \cdot (-1) &amp; 0 \cdot 4 + 1 \cdot 10 \\
  (-1) \cdot 1 + 5 \cdot (-1) &amp; (-1) \cdot 4 + 5 \cdot 10
  \end{array} } \right]
&amp;= 
\left[ {\begin{array}{cc}
-8 &amp; 108 \\
  -1 &amp; 10 \\
  -6 &amp; 46
  \end{array} } \right]
\end{aligned}`
$$

---

# Matrix Multiplication

- Properties of matrix multiplication

  - **Associative**: `\((\mathbf{A}\mathbf{B})\mathbf{C} = \mathbf{A}(\mathbf{B}\mathbf{C})\)`
  
  - **Distributive**: `\(\mathbf{A} (\mathbf{B} + \mathbf{C}) = \mathbf{A} \mathbf{B} + \mathbf{A} \mathbf{C}\)`
  
  - **NOT** commutative: `\(\mathbf{A}\mathbf{B} \neq \mathbf{B}\mathbf{A}\)`
  
  - **Transpose Rule**: `\((\mathbf{A} \mathbf{B})^\top = \mathbf{B}^\top \mathbf{A}^\top\)`

---

# Matrix Expectations

- Expectations are easily distributed throughout a matrix

$$
`\begin{aligned}
\mathbf{X} =\left[ {\begin{array}{ccc}
   x_{11} &amp; x_{12} &amp; x_{13} \\
   x_{21} &amp; x_{22} &amp; x_{23} \\
   x_{31} &amp; x_{32} &amp; x_{33}
  \end{array} } \right]
  \qquad 
  \text{; }
  \qquad
E(\mathbf{X}) = 
\left[ {\begin{array}{ccc}
   E(x_{11}) &amp; E(x_{12}) &amp; E(x_{13}) \\
   E(x_{21}) &amp; E(x_{22}) &amp; E(x_{23}) \\
   E(x_{31}) &amp; E(x_{32}) &amp; E(x_{33})
  \end{array} } \right]
\end{aligned}`
$$

---

# Matrix Derivatives

- Consider a matrix equation of the form `\(\mathbf{y} = \mathbf{A}\mathbf{x}\)`, meaning that each row is `\(y_i = a_{1i}x_1 + a_{2i}x_2 + \dots + a_{ki}x_k\)`

- In matrix notation:

$$
`\begin{aligned}
\left[ {\begin{array}{c}
   y_1\\
   y_2\\
   \vdots\\
   y_n
  \end{array} } \right]
  =
\left[ {\begin{array}{cccc}
      a_{11} &amp; a_{21} &amp; \dots &amp; a_{k1} \\
   a_{12} &amp; a_{22} &amp; \dots &amp; a_{k2} \\
   \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
   a_{1n} &amp; a_{2n} &amp; \dots &amp; a_{kn}
  \end{array} } \right]
  \left[ {\begin{array}{c}
   x_1\\
   x_2\\
   \vdots\\
   x_k
  \end{array} } \right]
\end{aligned}`
$$
--

- To take the partial derivative with respect to `\(\mathbf{x}\)`, we go element by element in `\(\mathbf{y}\)`: `\(\frac{\partial y_1}{\partial \mathbf{x}}\)`, `\(\frac{\partial y_2}{\partial \mathbf{x}}\)`, `\(\dots\)`, `\(\frac{\partial y_n}{\partial \mathbf{x}}\)`

--

- But to do THIS, we again go element by element through each value of `\(\mathbf{x}\)`, noting that `\(\frac{\partial y_1}{\partial x_1} = a_{11}\)` and `\(\frac{\partial y_1}{\partial x_2} = a_{21}\)`, and that `\(\frac{\partial y_2}{\partial x_1} = a_{12}\)` and `\(\frac{\partial y_2}{\partial x_2} = a_{22}\)`


---

# Matrix Derivatives

- We can write these in vector form as follows:

$$
`\begin{aligned}
\frac{\partial y_1}{\partial \mathbf{x}} =\left[ {\begin{array}{c}
   \frac{\partial y_1}{\partial x_1} \\
   \frac{\partial y_1}{\partial x_2} \\
   \vdots \\
   \frac{\partial y_1}{\partial x_k}
  \end{array} } \right] = \left[ {\begin{array}{c}
   a_{11}\\
   a_{21}\\
   \vdots \\
   a_{k1}
  \end{array} } \right]
  \text{; }~
\frac{\partial y_2}{\partial \mathbf{x}} =\left[ {\begin{array}{c}
   \frac{\partial y_2}{\partial x_1} \\
   \frac{\partial y_2}{\partial x_2} \\
   \vdots \\
   \frac{\partial y_2}{\partial x_k}
  \end{array} } \right] = \left[ {\begin{array}{c}
   a_{12}\\
   a_{22}\\
   \vdots \\
   a_{k2}
  \end{array} } \right]
  \text{; }
  ~\dots~
\frac{\partial y_n}{\partial \mathbf{x}} =\left[ {\begin{array}{c}
   \frac{\partial y_n}{\partial x_1} \\
   \frac{\partial y_n}{\partial x_2} \\
   \vdots \\
   \frac{\partial y_n}{\partial x_k}
  \end{array} } \right] = \left[ {\begin{array}{c}
   a_{1n}\\
   a_{2n}\\
   \vdots \\
   a_{kn}
  \end{array} } \right]
\end{aligned}`
$$
--

- Now let's just combine each of these vectors of derivatives into its own matrix to yield:

$$
`\begin{aligned}
\frac{\partial \mathbf{y}}{\partial \mathbf{x}} &amp;= \left[ {\begin{array}{cccc}
   a_{11} &amp; a_{12} &amp; \dots &amp; a_{1n} \\
   a_{21} &amp; a_{22} &amp; \dots &amp; a_{2n} \\
   \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
   a_{k1} &amp; a_{k2} &amp; \dots &amp; a_{kn}
  \end{array} } \right] = \mathbf{A}^\top
\end{aligned}`
$$

---

# Matrix Derivatives

- Thus `\(\frac{\partial \mathbf{y}}{\partial \mathbf{x}} = \frac{\partial (\mathbf{A}\mathbf{x})}{\partial \mathbf{x}} = \mathbf{A}^\top\)`

- From this, we can also note that, given `\(y = \mathbf{a}^\top \mathbf{x}\)`, `\(\frac{\partial y}{\partial \mathbf{x}} = \frac{\partial \mathbf{a}^\top \mathbf{x}}{\partial \mathbf{x}} = \mathbf{a}\)`

- And also, given `\(y = \mathbf{x}^\top \mathbf{A} \mathbf{x}\)`, `\(\frac{\partial y}{\partial \mathbf{x}} = \frac{\partial \mathbf{x}^\top \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} = 2\mathbf{A}\mathbf{x}\)`

- And finally, given `\(y = \mathbf{x}^\top \mathbf{A} \mathbf{x}\)`, `\(\frac{\partial y}{\partial \mathbf{A}} = \frac{\partial \mathbf{x}^\top \mathbf{A} \mathbf{x}}{\partial \mathbf{A}} = \mathbf{x}\mathbf{x}^\top\)`

---

# Special Matrices

- **Zero** matrices: `\(\mathbf{0}\)` has all entries as zero

  - NB: `\(\mathbf{A}_{r \times c} \cdot \mathbf{0}_{c \times n} = \mathbf{0}_{r \times n}\)` and `\(\mathbf{0}_{n \times r} \cdot \mathbf{A}_{r \times c} = \mathbf{0}_{n \times c}\)`

- **Square** matrices: `\(n \times n\)` size, meaning the same number of rows as columns

- **Symmetric** square matrices: `\(\mathbf{A} = \mathbf{A}^\top\)`

- **Diagonal** symmetric square matrices: zeros everywhere except the diagonal: if `\(i\)` are rows and `\(j\)` are columns, `\(i \neq j\)`, then `\(a_{ij} = 0\)`.

- **Identity** diagonal symmetric square matrices: `\(\mathbf{I}_n\)` is a diagonal matrix where the diagonals are 1s

  - What is
  
$$
`\begin{aligned}
\mathbf{A}=
\left[ {\begin{array}{ccc}
   99 &amp; 73 &amp; 2 \\
   13 &amp; 40 &amp; 41
  \end{array} } \right]
  \qquad
  \cdot
  \qquad
\mathbf{I}=
\left[ {\begin{array}{ccc}
   1 &amp; 0 &amp; 0 \\
   0 &amp; 1 &amp; 0 \\
   0 &amp; 0 &amp; 1
  \end{array} } \right]
\end{aligned}`
$$


---

# Matrix Inversion

- In the scalar world, we know we can rewrite a division problem `\(\frac{a}{b}\)` as a multiplication problem `\(a \times \frac{1}{b} = a \times b^{-1}\)`

  - `\(b^{-1}\)` is the inverse of `\(b\)`
  
  - The (obvious) requirement for the inverse is that `\(b \times b^{-1} = \frac{b}{1} \times \frac{1}{b} = \frac{b}{b} = 1\)`
  
- In the matrix world, the inverse of a matrix `\(\mathbf{A}\)` is denoted `\(\mathbf{A}^{-1}\)` and must also satisfy: `\(\mathbf{A}\mathbf{A}^{-1} = \mathbf{I}_n\)`

--

- Some properties!

  - If `\(\mathbf{C}\)` is an inverse of `\(\mathbf{A}\)`, then `\(\mathbf{A}\)` is also the inverse of `\(\mathbf{C}\)`

--

  - If `\(\mathbf{C}\)` and `\(\mathbf{D}\)` are both inverses of `\(\mathbf{A}\)`, then `\(\mathbf{C} = \mathbf{D}\)`

--

  - The inverse of an inverse of `\(\mathbf{A}\)` is just `\(\mathbf{A}\)`: `\((\mathbf{A}^{-1})^{-1} = \mathbf{A}\)`

--

  - The inverse of `\(\mathbf{A}^\top\)` is the same as the inverse of `\(\mathbf{A}\)`, transposed: `\((\mathbf{A}^\top)^{-1} = (\mathbf{A}^{-1})^\top\)`

--

  - If you have a scalar `\(c\)` multiplied by a matrix `\(\mathbf{A}\)`, then `\((c \mathbf{A})^{-1} = \frac{1}{c} \mathbf{A}^{-1}\)`
  
---

# Matrix Inversion

- To invert a `\(2\times 2\)` matrix, follow this rule:

- For 

$$
`\begin{aligned}
\mathbf{A} = 
\left[ {\begin{array}{cc}
   a &amp; b  \\
   c &amp; d
  \end{array} } \right]
\end{aligned}`
$$

- Invert using 

$$
`\begin{aligned}
\mathbf{A}^{-1} = \frac{1}{ad - bc} \left[ {\begin{array}{cc}
   d &amp; -b  \\
   -c &amp; a
  \end{array} } \right]
\end{aligned}`
$$

- where `\(ad - bc\)` is known as the **determinant** of the matrix `\(\mathbf{A}\)`, so named because it "determines" whether a matrix is invertible.

  - Why would it not be invertible? If `\(ad - bc = 0\)` or `\(ad = bc\)`!


---

# Matrix Inversion

- Matrix inversion gets harder with larger matrices...you can [learn](https://metric.ma.ic.ac.uk/metric_public/matrices/inverses/inverses2.html) how to do it manually, but this is where software like `R` comes in
handy!


```r
A &lt;- matrix(c(2, 1, 3, 4),
            nrow = 2,
            ncol = 2)
A
```

```
##      [,1] [,2]
## [1,]    2    3
## [2,]    1    4
```

--

- Use the `solve()` function to get the inverse of A

```r
A_inv &lt;- solve(A)
A_inv
```

```
##      [,1] [,2]
## [1,]  0.8 -0.6
## [2,] -0.2  0.4
```

---

# Matrix Math in `R`

- `R` also can make our lives easier for matrix multiplication...just use `%*%` instead of the standard `*`


```r
# Use %*% to do matrix multiplication
A*A_inv # Doesn't work...just does element-by-element multiplication
```

```
##      [,1] [,2]
## [1,]  1.6 -1.8
## [2,] -0.2  1.6
```

```r
A %*% A_inv # Works! We've proved that A_inv is the inverse of A!
```

```
##      [,1] [,2]
## [1,]    1    0
## [2,]    0    1
```

---

# Why all this!?

- It helps us solve systems of equations!
  
- Back in the day, you probably had lots of practice with these types of things:

$$
`\begin{aligned}
2 x_1 + x_2 &amp;= 10, \\
2 x_1 - x_2 &amp;= -10
\end{aligned}`
$$
- You probably learned to solve it various ways (i.e., solve for `\(x_1\)` first then plug in)

--

- We can solve with matrix math instead!

$$
`\begin{aligned}
\mathbf{A} &amp;= \begin{bmatrix}
2 &amp; 1 \\
2 &amp; -1
\end{bmatrix}, \\
\mathbf{x} &amp;= \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}, \\
\mathbf{b} &amp;= \begin{bmatrix} 10 \\ -10 \end{bmatrix}
\end{aligned}`
$$

---

# Systems of Equations

- We can re-write the two equations with matrix notation as `\(\mathbf{A}\mathbf{x} = \mathbf{b}\)`

- To solve for `\(\mathbf{x}\)`, we just invert `\(\mathbf{A}\)` and write `\(\mathbf{x} = \mathbf{A}^{-1}\mathbf{b}\)`

--


```r
A &lt;- matrix(c(2, 2, 1, -1),
            nrow = 2,
            ncol = 2)
b &lt;- matrix(c(10,-10),nrow = 2,ncol = 1)

solve(A)%*%b
```

```
##      [,1]
## [1,]    0
## [2,]   10
```

--

- `\(x_1 = 0\)` and `\(x_2 = 10\)`! So easy!

- Note that there is a unique solution for `\(x_1\)` and `\(x_2\)` iff `\(\mathbf{A}\)` is invertible

  - If not, there is either no solution or infinitely many solutions
  
---

# Multiple Regression (Thanks [PJE](https://as.nyu.edu/faculty/egan.html)!)

- We can use matrix algebra to help us with **multiple regression** (one outcome with multiple predictors)

  - Note: **multivariate regression** (multiple outcomes) `\(\neq\)` multiple regression

- Let's start with familiar notation and then break it down: `\(y_i = \beta_0 + \beta_1 x_{i} + \beta_2 z_{i} + u_i\)`

- What does `\(y\)` *look* like? I mean this literally...what is it in a dataset?

--

  - It is an `\(n\)`-length vector of values `\(\mathbf{y}\)`, one for each row in our dataset!
  
- `\(\mathbf{x}\)` and `\(\mathbf{z}\)` are the same


```
##     respondent_id           y            x            z
## 1               1  1.48840379 -1.270882210 -0.560475647
## 2               2  1.56929669  0.026706220 -0.230177489
## 3               3 -0.51183694  1.312016436  1.558708314
## 4               4  0.19565146 -0.277034208  0.070508391
## 5               5 -1.36595852 -0.822330832  0.129287735
## 6               6 -0.52127462  1.670037262  1.715064987
## 7               7 -1.57350731 -0.323988263  0.460916206
## 8               8 -2.26255920 -2.933003171 -1.265061235
## 9               9  1.27068095 -1.067079372 -0.686852852
## 10             10  0.86496848  0.473334639 -0.445661970
## 11             11 -0.45610173  0.648734835  1.224081797
## 12             12  0.85165175  0.967778149  0.359813827
## 13             13 -0.38540683 -1.217111258  0.400771451
## 14             14 -0.57162580  0.055120750  0.110682716
## 15             15 -0.47309995 -0.036433931 -0.555841135
## 16             16  1.97685029  2.088066499  1.786913137
## 17             17 -0.33548702  0.603526672  0.497850478
## 18             18 -1.36377198 -2.607323165 -1.966617157
## 19             19 -2.08597746 -0.148348444  0.701355902
## 20             20 -2.30884451 -1.496920198 -0.472791408
## 21             21 -0.45632688 -0.950177109 -1.067823706
## 22             22 -0.32948880 -1.165449529 -0.217974915
## 23             23  0.61929070 -1.516561892 -1.026004448
## 24             24  0.45149616 -0.984983421 -0.728891229
## 25             25  1.48020471  1.218822737 -0.625039268
## 26             26 -0.59219996 -2.338643212 -1.686693311
## 27             27 -0.46920989  1.073173617  0.837787044
## 28             28 -0.63925731  0.231333967  0.153373118
## 29             29 -0.07720614 -2.099993571 -1.138136937
## 30             30 -1.08690066  1.182506835  1.253814921
## 31             31  3.39984482  1.871015080  0.426464221
## 32             32  0.36118446  0.156432570 -0.295071483
## 33             33  0.25577175  0.936358583  0.895125661
## 34             34 -1.16102454  0.455636655  0.878133488
## 35             35 -2.62763591 -1.231666140  0.821581082
## 36             36 -0.18567892  1.819977468  0.688640254
## 37             37 -1.64356546 -0.906722417  0.553917654
## 38             38  1.15892992  0.678035800 -0.061911711
## 39             39  2.23340791  1.603140905 -0.305962664
## 40             40 -2.22542965 -1.824364162 -0.380471001
## 41             41 -0.08683764  0.007077356 -0.694706979
## 42             42 -0.76439621 -0.470114767 -0.207917278
## 43             43 -0.07608349 -2.837540511 -1.265396352
## 44             44 -2.65197127  0.654288312  2.168955965
## 45             45 -1.78058777 -0.393574175  1.207961998
## 46             46  1.37145530 -1.654015105 -1.123108583
## 47             47 -1.56273047 -1.864640420 -0.402884835
## 48             48 -0.67192393  0.221261419 -0.466655354
## 49             49  1.43533951  2.880074059  0.779965118
## 50             50 -0.80157050 -1.370399543 -0.083369066
## 51             51  0.41213598  1.041057361  0.253318514
## 52             52  0.20716588  0.740495486 -0.028546755
## 53             53 -0.01171466  0.289332122 -0.042870457
## 54             54 -0.91787996  0.360225676  1.368602284
## 55             55  1.47905616 -0.345223592 -0.225770986
## 56             56 -0.36896045  1.236075269  1.516470604
## 57             57  1.64378903 -0.985763271 -1.548752804
## 58             58  0.25831536  0.212174994  0.584613750
## 59             59  0.86333349  1.100827631  0.123854244
## 60             60 -1.90748286 -0.158639289  0.215941569
## 61             61  0.53159415  1.432350948  0.379639483
## 62             62 -1.53904746 -1.551500460 -0.502323453
## 63             63 -1.21300081 -1.593362628 -0.333207384
## 64             64  4.54123861  2.222464552 -1.018575383
## 65             65  1.87622139 -1.488648815 -1.071791226
## 66             66  1.84580865  0.601756233  0.303528641
## 67             67  0.50341871  1.084779453  0.448209779
## 68             68 -2.24030802 -0.430776399  0.053004227
## 69             69  0.12808218  1.439129512  0.922267468
## 70             70  0.45817175  2.419049213  2.050084686
## 71             71  0.62963250 -0.706411674 -0.491031166
## 72             72  1.02782100 -2.243875842 -2.309168876
## 73             73  0.65024218  0.971671271  1.005738524
## 74             74  0.73317755  1.419251136 -0.709200763
## 75             75  0.10830695 -1.429344713 -0.688008616
## 76             76 -1.54255348 -0.070424897  1.025571370
## 77             77  0.21259110 -0.246984608 -0.284773007
## 78             78  0.38503193 -0.910236963 -1.220717712
## 79             79  0.86469024  0.617826959  0.181303480
## 80             80 -0.43369035 -0.597256695 -0.138891362
## 81             81 -2.73080123 -1.057561948  0.005764186
## 82             82  1.99968114  1.648465577  0.385280401
## 83             83  0.03637618 -0.720310420 -0.370660032
## 84             84 -1.13116449 -0.221136314  0.644376549
## 85             85 -0.11813506 -0.456766131 -0.220486562
## 86             86 -0.06313725  0.134606070  0.331781964
## 87             87  1.33093976  2.206759303  1.096839013
## 88             88  1.72558346  0.519918783  0.435181491
## 89             89  0.53500341  0.428122200 -0.325931586
## 90             90 -0.33122663  0.649515601  1.148807618
## 91             91  1.38282918  1.207949166  0.993503856
## 92             92  0.72949511  0.223711048  0.548396960
## 93             93  1.23984664  0.333315263  0.238731735
## 94             94 -1.47283136 -1.523269434 -0.627906076
## 95             95  0.69168120  0.049850915  1.360652449
## 96             96  2.06391426  1.396953798 -0.600259587
## 97             97  2.46756067  2.788041817  2.187332993
## 98             98 -2.60217405  0.281339265  1.532610626
## 99             99 -0.59018233 -0.846866276 -0.235700359
## 100           100  0.06443449 -2.211900985 -1.026420900
```


---

# Multiple Regression

- Let's now look at the data in a different way, from the perspective of a single unit of observation

  - I.e., if we are dealing with a survey of individuals, our data might have some respondent `\(7\)` for whom we observe both `\(y_7\)` as well as `\(x_7\)` and `\(z_7\)`
  
- From this perspective, unit `\(7\)` is associated with an outcome `\(y_7\)` (a single value) and then a vector of predictors: `\(\mathbf{x}_7 = (x_7,z_7)\)`


```r
dat %&gt;% slice(7)
```

```
##   respondent_id         y          x         z
## 1             7 -1.573507 -0.3239883 0.4609162
```

--

- We can write our regression equation for this specific respondent as `\(y_7 = \beta_0 + \beta_1 x_7 + \beta_2 z_7 + u_7\)`, or we can write it as `\(y_7 = \mathbf{x}_7 \cdot \beta + u_7\)`

  - `\(\beta\)` is now itself a **vector** of coefficients: `\(\beta = (\beta_0,\beta_1,\beta_2)\)`
  
  - `\(\mathbf{x}_7\)` now needs to include the number 1: `\(\mathbf{x}_7 = (1,x_7,z_7)\)` in order to capture the `\(\beta_0\)` coefficient.


---

# Multiple Regression

- We can then think of `\(\beta\)` as a `\(k\times 1\)` vector (where `\(k\)` is the number of predictors) and `\(\mathbf{x}_7\)` as a `\(1 \times k\)` vector, and then matrix multiply them!

$$
`\begin{aligned}
y_7 &amp;= \mathbf{x}_7 \cdot \beta + u_7 \\
&amp;=  
  \left[ {\begin{array}{ccc}
   1 &amp; x_7 &amp; z_7
  \end{array} } \right]\cdot\left[ {\begin{array}{c}
   \beta_0 \\
   \beta_1 \\
   \beta_2
  \end{array} } \right] + u_7 \\
&amp;= \beta_0 + \beta_1 x_{7} + \beta_2 z_7 + u_7,
\end{aligned}`
$$
--

- Now this was just one observation in our data, but we can imagine doing this for every single row, and then stacking our equations on top of each other

$$
`\begin{aligned}
y_1 &amp;= \beta \cdot \mathbf{x}_1 + u_1, \\
y_2 &amp;= \beta \cdot \mathbf{x}_2 + u_2, \\
&amp;\vdots \\
y_n &amp;= \beta \cdot \mathbf{x}_N + u_n.
\end{aligned}`
$$

---

# Multiple Regression

- As with any system of equations, we can re-write as vectors and matrices

$$
`\begin{aligned}
\mathbf{y} = \left[ {\begin{array}{c}
   y_1  \\
   y_2 \\
   \vdots \\
   y_n
  \end{array} } \right]
\end{aligned}`
,
\qquad
\mathbf{X} =
`\begin{bmatrix}
  \mathbf{x}_{1} \\
  \mathbf{x}_{2} \\
  \vdots \\
  \mathbf{x}_{n}
\end{bmatrix}`
=
`\begin{bmatrix}
  1 &amp; x_{1} &amp; z_{1} \\
  1 &amp; x_{2} &amp; z_{2} \\
  \vdots &amp; \vdots \\
  1 &amp; x_{n} &amp; z_{n}
\end{bmatrix}`
,
\qquad
\mathbf{u} = \left[ {\begin{array}{c}
   u_1  \\
   u_2 \\
   \vdots \\
   u_n
  \end{array} } \right]
$$

- Plugging in: `\(\mathbf{y} = \mathbf{X}\cdot \beta + \mathbf{u}\)`

--

- Note that this is the same as writing:

$$
`\begin{aligned}
\begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix}_{n\times 1} &amp;= \begin{bmatrix}
  1 &amp; x_{1} &amp; z_{1} \\
  1 &amp; x_{2} &amp; z_{2} \\
  \vdots &amp; \vdots \\
  1 &amp; x_{n} &amp; z_{n}
\end{bmatrix}_{n\times k}\begin{bmatrix} \beta_0 \\ \beta_1 \\ \beta_2 \end{bmatrix}_{k \times 1} + \begin{bmatrix} u_1 \\ u_2 \\ \vdots \\ u_n \end{bmatrix}_{n \times 1}
\end{aligned}`
$$

- where `\(k\)` is the number of parameters (in this case, 3) and `\(n\)` is the number of observations

---

# Multiple Regression

- Note that `\(\mathbf{y} = \mathbf{X}\cdot \beta + \mathbf{u}\)` is assumed to be a reflection of the real world

  - Aside: prove to yourself that `\(\mathbf{y} = \mathbf{X}\cdot \beta + \mathbf{u}\)` and `\(\mathbf{y} =  \beta^\top\cdot\mathbf{X} + \mathbf{u}\)` are equivalent

--

- We estimate these, as before, with our OLS estimators `\(\hat{\beta}\)`

- To do so, we first calculate our residuals as `\(u = y - X\hat{\beta}\)`, and then add them up and square them.

  - In the **scalar** world, we would write this as `\(\sum u_i^2\)`.
  
  - In the **vector** world, we write this as `\(\mathbf{u}^\top \mathbf{u}\)`. Take a moment and try to see why!

--

$$
`\begin{aligned}
\mathbf{u}^\top \mathbf{u} = \begin{bmatrix} u_1 &amp; u_2 &amp; \dots &amp; u_n \end{bmatrix}_{1 \times n} \cdot \begin{bmatrix} u_1 \\ u_2 \\ \vdots \\ u_n \end{bmatrix}_{n \times 1} &amp;= \begin{bmatrix} u_1*u_1 + u_2*u_2 + \dots + u_n*u_n \end{bmatrix}_{1 \times n} = \sum u_i^2
\end{aligned}`
$$


---

# Multiple Regression

- We can re-write the sum of squared residuals as `\(\mathbf{u}^\top \mathbf{u} = (\mathbf{y} - \mathbf{X}\hat{\beta})^\top (\mathbf{y} - \mathbf{X}\hat{\beta})\)` by plugging in

--

- Now let's try doing some reorganizing of this

$$
`\begin{aligned}
(\mathbf{y} - \mathbf{X}\hat{\beta})^\top (\mathbf{y} - \mathbf{X}\hat{\beta}) &amp;= (\mathbf{y}^\top - \hat{\beta}^\top\mathbf{X}^\top)(\mathbf{y} - \mathbf{X}\hat{\beta})\\
&amp;= \mathbf{y}^\top \mathbf{y} - \mathbf{y}^\top \mathbf{X}\hat{\beta} - \hat{\beta}^\top \mathbf{X}^\top \mathbf{y} + \hat{\beta}^\top \mathbf{X}^\top \mathbf{X} \hat{\beta}
\end{aligned}`
$$
--

- To subtract, it must be that `\(\mathbf{y}^\top \mathbf{y}\)` is conformable with `\(\mathbf{y}^\top \mathbf{X}\hat{\beta}\)`, meaning they must have the same dimensions

--

- Note that `\(\mathbf{y}^\top \mathbf{y}\)` is a scalar, meaning that `\(\mathbf{y}^\top \mathbf{X}\hat{\beta}\)` must also be a scalar (by conformability)
  
  - Thus we can re-write `\(\mathbf{y}^\top \mathbf{X}\hat{\beta} = (\mathbf{y}^\top \mathbf{X}\hat{\beta})^\top = \hat{\beta}^\top \mathbf{X}^\top \mathbf{y}\)` (by transpose of a scalar)

--

- Substitute this in to reduce to:

$$
`\begin{aligned}
\mathbf{u}^\top \mathbf{u} &amp;= \mathbf{y}^\top \mathbf{y} - 2\hat{\beta}^\top \mathbf{X}^\top \mathbf{y} + \hat{\beta}^\top \mathbf{X}^\top \mathbf{X} \hat{\beta}
\end{aligned}`
$$

---

# Multiple Regression

- Take the derivative with respect to `\(\hat{\beta}\)` and set it equal to zero, just like we did in the bivariate case

$$
`\begin{aligned}
\frac{\partial \mathbf{u}^\top \mathbf{u}}{\partial \hat{\beta}} &amp;= -2\mathbf{X}^\top \mathbf{y} + 2 \mathbf{X}^\top \mathbf{X} \hat{\beta} = 0\\
(\mathbf{X}^\top \mathbf{X})\hat{\beta} &amp;= \mathbf{X}^\top \mathbf{y}
\end{aligned}`
$$

- To solve for `\(\hat{\beta}\)`, we need to pre-multiply both the left and the right by the inverse of `\((\mathbf{X}^\top \mathbf{X})\)`, assuming it exists

$$
`\begin{aligned}
(\mathbf{X}^\top \mathbf{X})\hat{\beta} &amp;= \mathbf{X}^\top \mathbf{y}\\
(\mathbf{X}^\top \mathbf{X})^{-1}(\mathbf{X}^\top \mathbf{X})\hat{\beta} &amp;= (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top \mathbf{y}\\
\mathbf{I}\hat{\beta} &amp;= (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top \mathbf{y}\\
\hat{\beta} &amp;= (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top \mathbf{y}\\
\end{aligned}`
$$
- **Welcome to the matrix definition of the OLS estimator!**

---

# Unbiasedness

- Is this unbiased?

- To start, let's fiddle with the preceding definition of `\(\hat{\beta}\)` a little bit by replacing `\(\mathbf{y}\)` with `\(\mathbf{X}\beta + \mathbf{u}\)`.

  - Note that this requires **Assumption 1**: that the population model can be written as `\(\mathbf{y} = \mathbf{X}\beta + \mathbf{u}\)`

$$
`\begin{aligned}
\hat{\beta} &amp;= (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top \mathbf{y}\\
 &amp;= (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top(\mathbf{X}\beta + \mathbf{u})\\
 &amp;= (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top\mathbf{X}\beta + (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top\mathbf{u}\\
 &amp;= \mathbf{I}\beta + (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top\mathbf{u}\\
 &amp;= \beta + (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top\mathbf{u}\\
\end{aligned}`
$$

---

# Unbiasedness

- Now let's invoke **Assumption 2** that these observations are drawn from an i.i.d. random sample, allowing us take expectations

$$
`\begin{aligned}
E(\hat{\beta}) &amp;= E\bigg[\beta + (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top\mathbf{u}\bigg]\\
 &amp;= E(\beta) + E\bigg[(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top\mathbf{u}\bigg]\\
&amp;= \beta + E\bigg[(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top\mathbf{u}\bigg]\\
\end{aligned}`
$$
- Note that this requires `\((\mathbf{X}^\top \mathbf{X})^{-1}\)` to exist, so we'll invoke **Assumption 3**: there is no perfect multicollinearity among our `\(X\)` values

  - *Compare this to the non-zero variance assumption invoked when we were working with scalars in the bivariate case*


---

# Unbiasedness

- Finally, let's invoke our most heroic assumption **Assumption 4**: `\(E(\mathbf{u}|\mathbf{X}) = \mathbf{0}\)`, and then rely on the law of iterated expectations (LIE)

$$
`\begin{aligned}
E(\hat{\beta}~|~\mathbf{X}) &amp;= \beta + E\bigg[(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top\mathbf{u}~|~\mathbf{X}\bigg]\\
 &amp;= \beta + (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top E(\mathbf{u}~|~\mathbf{X})\\
  &amp;= \beta + (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top \mathbf{0}\\
  &amp;= \beta
\end{aligned}`
$$

---

# Properties of the OLS Estimators

- `\(\mathbf{X}^\top \mathbf{u} = 0\)`: To prove, substitute the definition of `\(\mathbf{y} = \mathbf{X}\hat{\beta} + \mathbf{u}\)` into the normal equation

--

$$
`\begin{aligned}
(\mathbf{X}^\top \mathbf{X})\hat{\beta} &amp;= \mathbf{X}^\top \mathbf{y}\\
(\mathbf{X}^\top \mathbf{X})\hat{\beta} &amp;= \mathbf{X}^\top (\mathbf{X}\hat{\beta} + \mathbf{u})\\
(\mathbf{X}^\top \mathbf{X})\hat{\beta} &amp;= (\mathbf{X}^\top \mathbf{X})\hat{\beta} + \mathbf{X}^\top \mathbf{u}\\
0 &amp;= \mathbf{X}^\top \mathbf{u}
\end{aligned}`
$$

---

# Properties of the OLS Estimators

- If our regression specification includes a constant, `\(\sum u_i = 0\)`: To prove, look inside the matrices!

$$
`\begin{aligned}
\left[ {\begin{array}{ccc}
x_{11} &amp; x_{12} &amp; \dots &amp; x_{1n}  \\
x_{21} &amp; x_{22} &amp; \dots &amp; x_{2n}  \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
x_{k 1} &amp; x_{k2} &amp; \dots &amp; x_{kn}  \\
  \end{array} } \right]
\cdot
\left[ {\begin{array}{c}
u_1 \\
u_2 \\
\vdots \\
u_n \\
  \end{array} } \right]
=
\left[ {\begin{array}{c}
x_{11}*u_1 + x_{12}*u_2 + \dots + x_{1n}*u_n \\
x_{21}*u_1 + x_{22}*u_2 + \dots + x_{2n}*u_n \\
\vdots\\
x_{k1}*u_1 + x_{k2}*u_2 + \dots + x_{kn}*u_n \\ 
  \end{array} } \right]
=
\left[ {\begin{array}{c}
0\\
0\\
\vdots\\
0
  \end{array} } \right]
\end{aligned}`
$$
--

- If `\(\mathbf{X}^\top \mathbf{u} = \mathbf{0}\)`, then every column `\(\mathbf{x}_k\)`'s dot product with `\(\mathbf{u}\)` must be zero

- Since the first column of `\(\mathbf{X}\)` is all 1, then this first column reduces to `\(\sum u_i = 0\)`

--

- Also note that therefore `\(\bar{u} = 0\)` since `\(\bar{u} = \frac{\sum u_i}{n}\)`

---

# Properties of the OLS Estimators

- The regression **hyperplane** (no longer a single line, since we have multiple predictors) will pass through `\(\bar{X}\)` and `\(\bar{y}\)`

  - We just showed that `\(\bar{u} = 0\)`, and we know that `\(u = y - X\hat{\beta}\)`
  
  - Thus `\(\bar{u} = \bar{y} - \bar{x}\hat{\beta}\)`, meaning `\(\bar{y} = \bar{x}\hat{\beta}\)`
  
--

- The predicted values of `\(y\)` are uncorrelated with the residuals

  - `\(\hat{\mathbf{y}} = \mathbf{X}\hat{\beta}\)`, meaning that
  
$$
`\begin{aligned}
\hat{\mathbf{y}}^\top \mathbf{u} &amp;= \mathbf{X}\hat{\beta}^\top \mathbf{u}\\
&amp;= \hat{\beta}^\top \mathbf{X}^\top \mathbf{u}\\
&amp;= \hat{\beta}^\top \cdot \mathbf{0}
\end{aligned}`
$$

---

# Variance in matrix world

- Finally, let's calculate the variance of our OLS estimators, `\(\hat{\beta}\)`

- In the scalar world, we calculate the variance of a random variable as `\(\textit{var}(x) = E(x - E(x))^2\)`

- The matrix equivalent of this is called (confusingly) the **covariance** of a random vector, written `\(cov(\mathbf{x})\)`

  - Defined as `\(cov(\mathbf{x}) = E[(\mathbf{x} - E(\mathbf{x}))(\mathbf{x} - E(\mathbf{x}))^\top]\)`
  
- Let's write this out!

$$
`\begin{aligned}
cov(\mathbf{x}) &amp;= E\bigg\{\left[ {\begin{array}{c}
x_{1} - E(x_{1})\\
x_{2} - E(x_2) \\
\vdots \\
x_{n} - E(x_n) \\
  \end{array} } \right]
  \left[ {\begin{array}{cccc}
x_{1} - E(x_{1}) &amp; x_{2} - E(x_2) &amp; \dots &amp; x_{n} - E(x_n) \\
  \end{array} } \right]
  \bigg\}
\end{aligned}`
$$

---

# Variance in matrix world

$$
`\begin{aligned}
cov(\mathbf{x}) &amp;= E\bigg\{\left[ {\begin{array}{cccc}
(x_{1} - E(x_{1}))^2 &amp; (x_1 - E(x_1))(x_2 - E(x_2)) &amp; \dots &amp; (x_1 - E(x_1))(x_n - E(x_n))\\
(x_2 - E(x_2))(x_1 - E(x_1)) &amp; (x_{2} - E(x_{2}))^2 &amp; \dots &amp; (x_2 - E(x_2))(x_n - E(x_n)) \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
(x_n - E(x_n))(x_1 - E(x_1)) &amp; (x_n - E(x_n))(x_2 - E(x_2)) &amp; \dots &amp; (x_n - E(x_n))^2 \\
  \end{array} } \right]\bigg\}
\end{aligned}`
$$

- Distribute expectations throughout to get

$$
`\begin{aligned}
cov(\mathbf{x})    &amp;= \left[ {\begin{array}{cccc}
\sigma^2_{x_1} &amp; cov(x_1,x_2) &amp; \dots &amp; cov(x_1,x_n) \\
cov(x_2,x_1) &amp; \sigma^2_{x_2} &amp; \dots &amp; cov(x_2,x_n) \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
cov(x_n,x_1) &amp; cov(x_n,x_2) &amp; \dots &amp; \sigma^2_{x_n} \\
  \end{array} } \right]
\end{aligned}`
$$

- NB: this is called the covariance matrix of the random vector `\(\mathbf{x}\)`, AKA the **variance-covariance** matrix

  - Often depicted with `\(\mathbf{\Sigma}\)`

---

# Variance of `\(\hat{\beta}\)`

- So now let's use this to calculate the **variance of `\(\hat{\beta}\)`**

  - Note that we have already demonstrated that `\(E(\hat{\beta}) = \beta\)`
  
- Also note that `\(\hat{\beta} = \beta + (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{u}\)`, or `\(\hat{\beta} - \beta = (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{u}\)`

- Plug in

$$
`\begin{aligned}
E[(\hat{\beta} - \beta)(\hat{\beta}-\beta)^\top] &amp;= E\bigg[\bigg((\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{u}\bigg)\bigg((\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{u}\bigg)^\top\bigg]\\
&amp;= E\bigg[\bigg((\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{u}\bigg)\bigg(\mathbf{u}^\top\mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\bigg)\bigg]\\
&amp;= E\bigg[(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{u}\mathbf{u}^\top\mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\bigg]
\end{aligned}`
$$


---

# Errors

- This is the variance of our estimator: `\(E\bigg[(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{u}\mathbf{u}^\top\mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\bigg]\)`

- Taking a step back:

--

  - We have a **L**inear **E**stimator `\(\hat{\beta}\)`
  
  - We have proved it is **U**nbiased

- Is it the "best"? (Remember, **B**est **L**inear **U**nbiased **E**stimator is **BLUE**)

--

- To prove it is BLUE, we require **Assumption 5**: `\(E(\mathbf{u}\mathbf{u}^\top~|~\mathbf{X}) = \sigma^2\mathbf{I}\)`. AKA: "spherical errors"

--

  - a. **Homoskedasticity**: `\(var(u_1)  = var(u_2) = \dots = var(u_n) = \sigma^2\)`
  
  - b. **No autocorrelation**: `\(cov(u_i,u_j) = 0\)` for all `\(i \neq j\)`
  
---

# Errors

- Let's write out *Assumption 5**:

$$
`\begin{aligned}
E(\mathbf{u}\mathbf{u}^\top~|~\mathbf{X}) &amp;= E\bigg(\left[ {\begin{array}{c}
u_1~|~\mathbf{X}\\
u_2~|~\mathbf{X}\\
\vdots\\
u_n~|~\mathbf{X}\\
  \end{array} } \right] \left[ {\begin{array}{cccc}
u_1~|~\mathbf{X} &amp; u_2~|~\mathbf{X} &amp; \dots &amp;u_n~|~\mathbf{X}\\
  \end{array} } \right] \bigg)\\
&amp;= E\left[ {\begin{array}{cccc}
u_1^2~|~\mathbf{X} &amp; u_1u_2~|~\mathbf{X} &amp; \dots &amp; u_1u_n~|~\mathbf{X}\\
u_2u_1~|~\mathbf{X} &amp; u_2^2~|~\mathbf{X} &amp; \dots &amp; u_2u_n~|~\mathbf{X}\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
u_nu_1~|~\mathbf{X} &amp; u_nu_2~|~\mathbf{X} &amp; \dots &amp; u_n^2~|~\mathbf{X}\\
  \end{array} } \right]
\end{aligned}`
$$

---

# Errors

- Distribute expectations to get:

$$
`\begin{aligned}
E(\mathbf{u}\mathbf{u}^\top~|~\mathbf{X}) &amp;= \left[ {\begin{array}{cccc}
E(u_1^2~|~\mathbf{X}) &amp; E(u_1u_2~|~\mathbf{X}) &amp; \dots &amp; E(u_1u_n~|~\mathbf{X})\\
E(u_2u_1~|~\mathbf{X}) &amp; E(u_2^2~|~\mathbf{X}) &amp; \dots &amp; E(u_2u_n~|~\mathbf{X})\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
E(u_nu_1~|~\mathbf{X}) &amp; E(u_nu_2~|~\mathbf{X}) &amp; \dots &amp; E(u_n^2~|~\mathbf{X})\\
  \end{array} } \right]
\end{aligned}`
$$

--

- From **Assumption 5**:

  - Homoskedasticity states that the variance of `\(u_i = \sigma^2\)` for all `\(i\)`, or `\(\textit{VAR}(u_i | \mathbf{X}) = \sigma^2~~ \forall~i\)`
  
  - No autocorrelation states that `\(cov(u_i,u_j | \mathbf{X}) = 0\)`
  
---

# Errors

- Thus, assumption 5 allows us to re-write:

$$
`\begin{aligned}
E(\mathbf{u}\mathbf{u}^\top~|~\mathbf{X}) &amp;= \left[ {\begin{array}{cccc}
E(u_1^2~|~\mathbf{X}) &amp; E(u_1u_2~|~\mathbf{X}) &amp; \dots &amp; E(u_1u_n~|~\mathbf{X})\\
E(u_2u_1~|~\mathbf{X}) &amp; E(u_2^2~|~\mathbf{X}) &amp; \dots &amp; E(u_2u_n~|~\mathbf{X})\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
E(u_nu_1~|~\mathbf{X}) &amp; E(u_nu_2~|~\mathbf{X}) &amp; \dots &amp; E(u_n^2~|~\mathbf{X})\\
  \end{array} } \right]\\
  &amp;= \left[ {\begin{array}{cccc}
\sigma^2 &amp; 0 &amp; \dots &amp; 0\\
0 &amp; \sigma^2 &amp; \dots &amp; 0\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \dots &amp; \sigma^2\\
  \end{array} } \right]
\end{aligned}`
$$
- which is the same as writing `\(\sigma^2\mathbf{I}\)`



---

# Variance of `\(\hat{\beta}\)`

- So we have `\(E\bigg[(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{u}\mathbf{u}^\top\mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\bigg]\)`

- Take the LIE conditional on `\(\mathbf{X}\)` to get

$$
`\begin{aligned}
E[(\hat{\beta} - \beta)(\hat{\beta}-\beta)^\top~|~\mathbf{X}] &amp;= E\bigg[(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{u}\mathbf{u}^\top\mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}~\bigg|~\mathbf{X}\bigg]\\
 &amp;= (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top E(\mathbf{u}\mathbf{u}^\top~|~\mathbf{X})\mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\\
 &amp;= (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top~~(\sigma^2\mathbf{I})~~\mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\\
 &amp;= \sigma^2\mathbf{I}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\\
 &amp;= \sigma^2\mathbf{I}\mathbf{I}(\mathbf{X}^\top\mathbf{X})^{-1}\\
 &amp;= \sigma^2(\mathbf{X}^\top\mathbf{X})^{-1}\\
\end{aligned}`
$$


---

# What does this give us?

- The OLS estimator -- `\(\hat{\beta}\)` -- is a random vector, distributed with mean `\(\beta\)` and a variance-covariance matrix `\(\sigma^2(\mathbf{X}^\top\mathbf{X})^{-1}\)`

- We might be particularly interested in just one of the coefficients contained within this vector (i.e., `\(\beta_1\)` speaks to a theoretical quantity of interest, while the other `\(\beta_2, \beta_3, \dots, \beta_k\)` are controls)

- To find the mean of `\(\hat{\beta}_1\)`, we look inside our vector of expected values of `\(\hat{\beta}\)` and extract the element corresponding to `\(E(\hat{\beta}_1) = (\mathbf{X}^\top\mathbf{X}_1^{-1}\mathbf{X}_1^\top\mathbf{y})\)`

- To find the variance of `\(\hat{\beta}_1\)`, we look inside our variance-covariance matrix `\(cov(\hat{\beta}) = \mathbf{\Sigma}_{\hat{\beta}}\)` and extract the entry corresponding to `\(E(\hat{\beta}_1 - E(\hat{\beta}_1))^2 = \sigma^2(\mathbf{X}^\top\mathbf{X})^{-1}_{11}\)`

- As always, we never know `\(\sigma^2\)`, meaning we never really know `\(var(\hat{\beta})\)`

- In practice, we estimate the unknown `\(\sigma^2\)` with `\(\hat{\sigma}^2 = \frac{\mathbf{u}^\top\mathbf{u}}{n - k}\)`

--

  - Note that we are assuming `\(k\)` includes `\(\beta_0\)`. If not, we write as `\(\hat{\sigma}^2 = \frac{\mathbf{u}^\top\mathbf{u}}{n - k - 1}\)`

---

# A few final comments

- As in the univariate and bivariate cases, we can appeal to the **C**entral **L**imit **T**heorem (CLT) to assume that the sampling distribution of `\(\hat{\beta} \overset{d}{\rightarrow} \textit{MVN}(\beta,\sigma^2(\mathbf{X}^\top\mathbf{X})^{-1})\)`

--

  - The symbol `\(\overset{d}{\rightarrow}\)` means "distributed asymptotically as"
  
--

- The multivariate normal (MVN) joint distribution means that we can extract any element of `\(\hat{\beta}\)` and standardize it, and it will be distributed asymptotically as the standard normal

--

  - I.e., `\(\frac{\hat{\beta}_k - \bar{\beta}_k}{\sqrt{\hat{\sigma}^2(\mathbf{X}^\top\mathbf{X})^{-1}_{kk}}} \overset{d}{\rightarrow}\mathcal{N}(0,1)\)`
  
  - NB: the statistic `\(\frac{\hat{\beta}_k - \bar{\beta}_k}{\sqrt{\hat{\sigma}^2(\mathbf{X}^\top\mathbf{X})^{-1}_{kk}}}\)` is distributed according to the Student's `\(t\)` distribution with `\(N-K-1\)` degrees of freedom: `\(\frac{\hat{\beta}_k - \bar{\beta}_k}{\sqrt{\hat{\sigma}^2(\mathbf{X}^\top\mathbf{X})^{-1}_{kk}}} \sim t_{N-k-1}\)`

--

- In small samples, we make **one more assumption** that the errors are normally distributed

---

# FWL and Partialling Out

- To understand what multiple regression looks like in matrix form, we need some helper concepts

- The "residual maker" is a matrix `\(\mathbf{M}\)` that, when multiplied by `\(\mathbf{y}\)`, creates **residuals** `\(\mathbf{u}\)`

- Start with the definition of the residual: `\(\mathbf{u} = \mathbf{y} - \hat{\mathbf{y}}\)` and substitute `\(\hat{\mathbf{y}} = \mathbf{X}\hat{\beta}\)` in

$$
`\begin{aligned}
\mathbf{u} &amp;= \mathbf{y} - \mathbf{X}\hat{\beta}
\end{aligned}`
$$
- Now replace `\(\hat{\beta}\)` with the definition `\((\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{y}\)`

$$
`\begin{aligned}
\mathbf{u} &amp;= \mathbf{y} - \mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{y}\\
&amp;= (\mathbf{I} - \mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top)\mathbf{y}\\
&amp;= \mathbf{M}\mathbf{y}
\end{aligned}`
$$

---

# FWL and Partialling Out

- `\(\mathbf{M}\)` is super helpful. It is both square and **idempotent**, meaning that `\(\mathbf{M}\mathbf{M} = \mathbf{M}\)`. (Try proving this for yourself!)

- It also has the properties:

  1. `\(\mathbf{M}\mathbf{X} = \mathbf{0}\)`
  
  2. `\(\mathbf{M}\mathbf{u} = \mathbf{u}\)`

---

# FWL and Partialling Out

- The "hat" matrix is a matrix `\(\mathbf{H}\)` that, when multiplied by `\(\mathbf{y}\)`, creates **predicted values** `\(\mathbf{\hat{y}}\)`

$$
`\begin{aligned}
\hat{\mathbf{y}} &amp;= \mathbf{y} - \mathbf{u} \\
&amp;= (\mathbf{I} - \mathbf{M})\mathbf{y} \\
&amp;= \mathbf{H}\mathbf{y}
\end{aligned}`
$$
- So we now have `\(\mathbf{M} = \mathbf{I} - \mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\)` and `\(\mathbf{H} = \mathbf{I} - \mathbf{M}\)`

- But this just means `\(\mathbf{H} = \mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\)`

---

# FWL and Partialling Out

- These "hat" and "residual maker" matrices can help us understand OVB and, more generally, what "controlling" for a variable means in the matrix world

- Consider the classic example where the true regression equation is given by `\(\mathbf{y} = \mathbf{x}_1\beta_1 + \mathbf{x}_2\beta_2 + u\)`, but we mistakenly omit `\(\mathbf{x}_2\)`

- The true normal equation is:

$$
`\begin{aligned}
\left[ {\begin{array}{cc}
\mathbf{x}^\top_1\mathbf{x}_1 &amp; \mathbf{x}^\top_1\mathbf{x}_2 \\
\mathbf{x}^\top_2\mathbf{x}_1 &amp; \mathbf{x}^\top_2\mathbf{x}_2 
  \end{array} } \right]^{-1}\cdot 
  \left[ {\begin{array}{c}
\mathbf{x}_1\mathbf{y} \\
\mathbf{x}_2\mathbf{y}
  \end{array} } \right] &amp;= 
  \left[ {\begin{array}{c}
\hat{\beta}_1 \\
\hat{\beta}_2
  \end{array} } \right]
\end{aligned}`
$$

---

# FWL and Partialling Out

- First, solve for `\(\hat{\beta}_1\)`

$$
`\begin{aligned}
(\mathbf{x}_1^\top\mathbf{x}_1)\hat{\beta}_1 + (\mathbf{x}_1^\top\mathbf{x}_2)\hat{\beta}_2 &amp;= \mathbf{x}_1^\top\mathbf{y}\\
(\mathbf{x}_1^\top\mathbf{x}_1)\hat{\beta}_1 &amp;= \mathbf{x}_1^\top\mathbf{y} -  (\mathbf{x}_1^\top\mathbf{x}_2)\hat{\beta}_2\\
\hat{\beta}_1 &amp;= (\mathbf{x}_1^\top\mathbf{x}_1)^{-1}\mathbf{x}_1^\top\mathbf{y} - (\mathbf{x}_1^\top\mathbf{x}_1)^{-1}(\mathbf{x}_1^\top\mathbf{x}_2)\hat{\beta}_2\\
\hat{\beta}_1 &amp;= (\mathbf{x}_1^\top\mathbf{x}_1)^{-1}\mathbf{x}_1^\top(\mathbf{y} - \mathbf{x}_2\hat{\beta}_2)
\end{aligned}`
$$

- Recognize this? 

--

  - `\((\mathbf{x}^\top_1\mathbf{x}_1)^{-1}\mathbf{x}^\top_1\mathbf{x}_2\)` is the regression of `\(\mathbf{x}_2\)` on `\(\mathbf{x}_1\)`. This will be zero if `\(\mathbf{x}_2\)` is unrelated to `\(\mathbf{x}_1\)`
  
  - `\(\hat{\beta}_2\)` is the relationship between `\(\mathbf{y}\)` and `\(\mathbf{x}_2\)`.
  
--

- **This is just OVB in matrix form**

---

# FWL and Partialling Out

- Now let's see what happens when we control for `\(\mathbf{x}_2\)`

- Start with `\(\hat{\beta}_1 = (\mathbf{x}^\top_1\mathbf{x}_1)^{-1}\mathbf{x}^\top_1(\mathbf{y} - \mathbf{x}_2\hat{\beta}_2)\)`

- Then do direct multiplication on the second row in 

$$
`\begin{aligned}
\left[ {\begin{array}{cc}
\mathbf{x}^\top_1\mathbf{x}_1 &amp; \mathbf{x}^\top_1\mathbf{x}_2 \\
\mathbf{x}^\top_2\mathbf{x}_1 &amp; \mathbf{x}^\top_2\mathbf{x}_2 
  \end{array} } \right]^{-1}\cdot 
  \left[ {\begin{array}{c}
\mathbf{x}_1\mathbf{y} \\
\mathbf{x}_2\mathbf{y}
  \end{array} } \right] &amp;= 
  \left[ {\begin{array}{c}
\hat{\beta}_1 \\
\hat{\beta}_2
  \end{array} } \right]
\end{aligned}`
$$
- to yield `\(\mathbf{x}_2^\top\mathbf{x}_1\hat{\beta}_1 + \mathbf{x}_2^\top\mathbf{x}_2\hat{\beta}_2 = \mathbf{x}_2^\top\mathbf{y}\)`

- Finally, substitute in our definition of `\(\hat{\beta}_1\)` to get `\(\mathbf{x}_2^\top\mathbf{x}_1(\mathbf{x}_1^\top\mathbf{x}_1)^{-1}\mathbf{x}_1^\top\mathbf{y} - \mathbf{x}_2^\top\mathbf{x}_1(\mathbf{x}_1^\top\mathbf{x}_1)^{-1}\mathbf{x}_1^\top\mathbf{x}_2\hat{\beta}_2 + \mathbf{x}_2^\top\mathbf{x}_2\hat{\beta}_2 = \mathbf{x}_2^\top\mathbf{y}\)`

---

# FWL and Partialling Out

- So this is horrible, but try this!

$$
`\begin{aligned}
\mathbf{x}_2^\top\mathbf{y} - \mathbf{x}_2^\top\mathbf{x}_1(\mathbf{x}_1^\top\mathbf{x}_1)^{-1}\mathbf{x}_1^\top\mathbf{y} &amp;= \mathbf{x}_2^\top\mathbf{x}_2\hat{\beta}_2 - \mathbf{x}_2^\top\mathbf{x}_1(\mathbf{x}_1^\top\mathbf{x}_1)^{-1}\mathbf{x}_1^\top\mathbf{x}_2\hat{\beta}_2\\
\mathbf{x}_2^\top\mathbf{y} - \mathbf{x}_2^\top\mathbf{x}_1(\mathbf{x}_1^\top\mathbf{x}_1)^{-1}\mathbf{x}_1^\top\mathbf{y} &amp;= [\mathbf{x}_2^\top\mathbf{x}_2 - \mathbf{x}_2^\top\mathbf{x}_1(\mathbf{x}_1^\top\mathbf{x}_1)^{-1}\mathbf{x}_1^\top\mathbf{x}_2]\hat{\beta}_2\\
\mathbf{x}_2^\top\mathbf{y} - \mathbf{x}_2^\top\mathbf{x}_1(\mathbf{x}_1^\top\mathbf{x}_1)^{-1}\mathbf{x}_1^\top\mathbf{y} &amp;= [(\mathbf{x}_2^\top - \mathbf{x}_2^\top\mathbf{x}_1(\mathbf{x}_1^\top\mathbf{x}_1)^{-1}\mathbf{x}_1^\top)\mathbf{x}_2]\hat{\beta}_2\\
\mathbf{x}_2^\top\mathbf{y} - \mathbf{x}_2^\top\mathbf{x}_1(\mathbf{x}_1^\top\mathbf{x}_1)^{-1}\mathbf{x}_1^\top\mathbf{y} &amp;= [(\mathbf{x}_2^\top(\mathbf{I} - \mathbf{x}_1(\mathbf{x}_1^\top\mathbf{x}_1)^{-1}\mathbf{x}_1^\top)\mathbf{x}_2]\hat{\beta}_2\\
(\mathbf{x}_2^\top - \mathbf{x}_2^\top\mathbf{x}_1(\mathbf{x}_1^\top\mathbf{x}_1)^{-1}\mathbf{x}_1^\top)\mathbf{y} &amp;= [(\mathbf{x}_2^\top(\mathbf{I} - \mathbf{x}_1(\mathbf{x}_1^\top\mathbf{x}_1)^{-1}\mathbf{x}_1^\top)\mathbf{x}_2]\hat{\beta}_2\\
\mathbf{x}_2^\top(\mathbf{I} - \mathbf{x}_1(\mathbf{x}_1^\top\mathbf{x}_1)^{-1}\mathbf{x}_1^\top)\mathbf{y} &amp;= [(\mathbf{x}_2^\top(\mathbf{I} - \mathbf{x}_1(\mathbf{x}_1^\top\mathbf{x}_1)^{-1}\mathbf{x}_1^\top)\mathbf{x}_2]\hat{\beta}_2\\
\hat{\beta}_2 &amp;= [(\mathbf{x}_2^\top(\mathbf{I} - \mathbf{x}_1(\mathbf{x}_1^\top\mathbf{x}_1)^{-1}\mathbf{x}_1^\top)\mathbf{x}_2]^{-1}\mathbf{x}_2^\top(\mathbf{I} - \mathbf{x}_1(\mathbf{x}_1^\top\mathbf{x}_1)^{-1}\mathbf{x}_1^\top)\mathbf{y}\\
&amp;= (\mathbf{x}_2^\top\mathbf{M}_1\mathbf{x}_2)^{-1}(\mathbf{x}_2^\top\mathbf{M}_1\mathbf{y})
\end{aligned}`
$$

---

# FWL and Partialling Out

- So we now have `\(\hat{\beta}_2 = (\mathbf{x}_2^\top\mathbf{M}_1\mathbf{x}_2)^{-1}(\mathbf{x}_2^\top\mathbf{M}_1\mathbf{y})\)`

- Remember that `\(\mathbf{M}\)` is the residual maker, meaning that `\(\mathbf{M}_1\)` is making residuals for regressions on the `\(\mathbf{x}_1\)` variables

--

  - `\(\mathbf{M}_1\mathbf{y}\)` therefore creates residuals from regressing `\(\mathbf{y}\)` on `\(\mathbf{x}_1\)`
  
  - `\(\mathbf{M}_1\mathbf{x}_2\)` therefore creates residuals from regressing `\(\mathbf{x}_2\)` on `\(\mathbf{x}_1\)`
  
--

- Since `\(\mathbf{M}\)` is both idempotent and symmetric, we can rewrite as `\(\hat{\beta}_2 = (\mathbf{x}^{*\top}_2\mathbf{x}_2)^{-1}\mathbf{x}^{*\top}_2\mathbf{y}^*\)`

  - Where `\(\mathbf{x}_2^* = \mathbf{M}_1\mathbf{x}_2\)` and `\(\mathbf{y}^* = \mathbf{M}_1\mathbf{y}\)`
  
--

- This leads to the **Frisch-Waugh-Lovell** Theorem: In the OLS regression of a vector `\(\mathbf{y}\)` on two sets of variables `\(\mathbf{x}_1\)` and `\(\mathbf{x}_2\)`, `\(\hat{\beta}_2\)` is the coefficient obtained when the residuals from a regression of `\(\mathbf{y}\)` on `\(\mathbf{x}_1\)` alone are regressed on the set of residuals obtained when `\(\mathbf{x}_2\)` is regressed on `\(\mathbf{x}_1\)`

---

# FWL and Partialling Out

- Imagine the following model (reverting back to the layperson's notation here): `\(Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + u\)`

- According to FWL:

  1. Regress `\(Y\)` on `\(X_1\)` and obtain the residuals `\(\hat{u}_1\)` (i.e., `\(\mathbf{M}_1\mathbf{y}\)` in matrix notation)
  
  2. Regress `\(X_2\)` on `\(X_1\)` and obtain the residuals `\(\hat{u}_2\)` (i.e., `\(\mathbf{M}_1\mathbf{x}_2\)` in matrix notation)
  
  3. Regress `\(X_3\)` on `\(X_1\)` and obtain the residuals `\(\hat{u}_3\)` (i.e., `\(\mathbf{M}_1\mathbf{x}_3\)` in matrix notation)
  
  4. Regress `\(\hat{u}_1\)` on `\(\hat{u}_2\)` and `\(\hat{u}_3\)`: `\(\hat{u}_1 = \rho_0 + \rho_1 \hat{u}_2 + \rho_2 \hat{u}_3 + \epsilon\)`
  
--

- `\(\hat{\beta}_2\)` will be equal to `\(\hat{\rho}_1\)` and `\(\hat{\beta}_3\)` will be equal to `\(\hat{\rho}_3\)`!

--

- Steps 2 and 3 are called "partialling out" or "netting out" the effect of `\(X_1\)`. For this reason, the coefficients in multiple regression are often referred to as "partial regression coefficients". 

---

# FWL and Partialling Out

- Let's try it!


```r
X1 &lt;- rnorm(100)
X2 &lt;- rnorm(100)
X3 &lt;- rnorm(100)

# True beta_1 = 1, beta_2 = -1, beta_3 = 3
Y &lt;- X1 - X2 + 3*X3 + rnorm(100) 

# Multiple regression
mFull &lt;- lm(Y ~ X1 + X2 + X3)

# FWL way
u_1 &lt;- resid(lm(Y ~ X1))
u_2 &lt;- resid(lm(X2 ~ X1))
u_3 &lt;- resid(lm(X3 ~ X1))
mRes &lt;- lm(u_1 ~ u_2 + u_3)
```

---

# FWL and Partialling Out

- As promised, we get the same estimates for `\(\hat{\beta}_2\)` and `\(\hat{\beta}_3\)` whether we estimate them in the standard multiple regression setting, or if we use the FWL residualizer approach


```r
# Same coefficients!
round(coef(mFull)[c(3,4)],4)
```

```
##      X2      X3 
## -0.9926  2.9206
```

```r
round(coef(mRes)[c(2,3)],4)
```

```
##     u_2     u_3 
## -0.9926  2.9206
```




    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="libs/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "17:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
