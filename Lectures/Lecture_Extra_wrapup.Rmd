---
title: "Lecture Wrapup"
subtitle: "Quantitative Political Science"
author: "Prof. Bisbee"
institute: "Vanderbilt University"
date: "Lecture Date: 2024/12/03\n Slides Updated: `r Sys.Date()`"
output:
  xaringan::moon_reader:
    # self_contained: true
    chakra: libs/remark-latest.min.js
    lib_dir: libs
    css:
      - default
      - css/lexis.css
      - css/lexis-fonts.css
    #seal: false
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "17:9"

---

```{css,echo = F}
.small .remark-code { /*Change made here*/
  font-size: 85% !important;
}
.tiny .remark-code { /*Change made here*/
  font-size: 50% !important;
}
```

```{r,include=F}
set.seed(123)
options(width=60)
knitr::opts_chunk$set(fig.align='center',fig.width=9,fig.height=5)
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
require(tidyverse)
require(ggpubr)
```

# Agenda

1. Variance of OLS estimators

2. Heteroskedasticity

3. Controlling for a variable

4. Odds and ends

---

# Recap

- We went over 4 assumptions to characterize the bias of our OLS estimators

1. Relationship between $x$ and $y$ is **linear in its parameters**

2. $x$ and $y$ are drawn from a random sample, making them **i.i.d.**

3. $VAR(X) \neq 0$

4. $E(u|x) = 0$

- With these, we demonstrated that $\hat{\beta}_0$ and $\hat{\beta}_1$ are **unbiased** for $\beta_0$ and $\beta_1$

---

# Sampling Distributions

- $\hat{\beta}_0$ and $\hat{\beta}_1$ are statistics, just like $\bar{Y}$

- When we evaluate statistics, we care about both their **bias** (last class) and their **variance**

  - How far can we expect them to be from their true value (i.e., the population parameter) on average?
  
- In the univariate case, we were interested in the **sampling distribution** of $\bar{Y}$

- Here, we are also interested in the sampling distributions of $\hat{\beta}_0$ and $\hat{\beta}_1$

---

# Variance

- We already know the means of $\hat{\beta}_0$ and $\hat{\beta}_1$: they are $\beta_0$ and $\beta_1$ (from last class)

- To compute the **variances** of $\hat{\beta}_0$ and $\hat{\beta}_1$, we need a **fifth assumption**

Assumption 5: $VAR(u|x) = \sigma^2$

  - The error has the **same variance** regardless of the value of $x$
  
- This is known as **homoskedasticity**

- If this fails, we have **heteroskedasticity**

---

# Variance

```{r,message=F,warning =F}
require(tidyverse)
set.seed(123)
X <- rnorm(500,mean = 12,sd = 4)
Y <- rnorm(500,mean = X,sd = X/3)
Y2 <- rnorm(500,mean = X,sd = 2)

p <- data.frame(X = X,Heteroskedastic = Y,Homoskedastic = Y2) %>%
  gather(outcome,value,-X) %>%
  mutate(outcome = factor(outcome,levels = c('Homoskedastic','Heteroskedastic'))) %>%
  ggplot(aes(x = X,y = value)) + 
  geom_point() + 
  geom_smooth(method = 'lm') + 
  facet_wrap(~outcome)
```

---

# Variance

```{r,message=F,warning =F}
p
```

---

# Variance

- Note the difference between Assumptions 4 and 5!

  - $E(u|x) = 0$
  
  - $VAR(u|x) = \sigma^2$
  
- We don't need assumption 5 for unbiasedness, but we do for variance!

- What is $VAR(y|x)$?

---

# Variance

$$
\begin{aligned}
VAR(y|x) &= VAR(\beta_0 + \beta_1 x + u | x)\\
&= VAR(\beta_0 | x) + VAR(\beta_1 x | x) + VAR(u | x)\\
&= 0 + 0 + \sigma^2\\
&= \sigma^2
\end{aligned}
$$
- What is $\sigma^2$?

--

- It is a measure of the **extent to which unexplained factors are affecting $y$**

  - These factors are not related to $x$ (from assumption 4)
  
  - These factors are constant regardless of $x$ (from assumption 5)
  
  - When $\sigma^2$ is big, it means that other factors explain a lot of variation in $y$ beyond just $x$
  
  - When $\sigma^2$ is small, it means that $x$ explains a lot of variation in $y$
  
- Note that $\sigma^2$ is a **parameter**, something that exists in the population
  
---

# Variance of estimators

- $VAR(\hat{\beta}_0) = \frac{\sigma^2 \frac{\sum x_i^2}{n}}{SST_x}$ and $VAR(\hat{\beta}_1) = \frac{\sigma^2}{SST_x}$

- I'll leave it to you to prove $VAR(\hat{\beta}_0)$, but let's dig into $VAR(\hat{\beta}_1)$

$$
\begin{aligned}
\hat{\beta}_1 &= \beta_1 + \frac{\sum(x_i - \bar{x})u_i}{SST_x} \\
VAR(\hat{\beta}_1|x) &= VAR\bigg[\beta_1 + \frac{\sum(x_i - \bar{x})u_i}{SST_x}~\bigg|~x\bigg]\\
&= VAR(\beta_1|x) + \frac{1}{SST_x^2}\sum(x_i-\bar{x})^2VAR(u_i~|~x)\\
&= 0 + \frac{SST_x}{SST_x^2}VAR(u_i|x)\\
&= \frac{\sigma^2}{SST_x}
\end{aligned}
$$

---

# Sampling Variance of $\hat{\beta}_1$

- So $VAR(\hat{\beta}_1) = \frac{\sigma^2}{SST_x}$

- Want this to be as small as possible (bias-variance tradeoff)

- As $\sigma^2$ gets smaller, so does $VAR(\hat{\beta}_1)$

- As $SST_x$ gets bigger, $VAR(\hat{\beta}_1)$ gets smaller

- Unpack $SST_x$ for more insights!

---

# Sampling Variance of $\hat{\beta}_1$

$$
\begin{aligned}
SST_x &= \sum(x_i - \bar{x})^2\\
\frac{SST_x}{n} &= \frac{\sum(x_i - \bar{x})^2}{n}\\
&= var(x) \text{ sample variance of }x\\
SST_x &= n*var\\
VAR(\hat{\beta}_1) &= \frac{\sigma^2}{n*var(x)}
\end{aligned}
$$
- What can we actually manipulate here as a researcher?

  - $\sigma^2$ is a parameter: it declines when $x$ explains $y$ well. But we don't have a lot of control over this.
  
  - $var(x)$ is the empirical variance of $x$ in our sample. It approximates the population variance, but we don't have a ton of control over this either.
  
  - $n$: we choose this!
  
---

# Heteroskedasticity

- The preceding results rely on the assumption of $VAR(u|x) = \sigma^2$

- What if this doesn't hold?

```{r,message=F,warning =F}
p
```

---

# Heteroskedasticity

- We would then say that the variance of the errors conditional on $x$ is specific to that unit

  - $VAR(u_i|x_i) = \sigma_i^2$
  
- Recall that $\hat{\beta}_1 = \frac{\sum(x_i - \bar{x})u_i}{\sum(x_i - \bar{x})^2}$ and that 

$$
\begin{aligned}
VAR(\hat{\beta}_1) &= VAR\bigg[\frac{\sum(x_i - \bar{x})u_i}{\sum(x_i - \bar{x})^2}\bigg]\\
&= \frac{1}{SST_x^2}\sum(x_i - \bar{x})^2 VAR(u_i|x_i)\\
&= \frac{\sum(x_i - \bar{x})^2\sigma_i^2}{SST_x^2}
\end{aligned}
$$


---

# Heteroskedasticity

- What to do?

- In 1980, one of the most cited economics papers was written by Halbert White

- In it, he proposed calculating heteroskedastic robust standard errors as $\widehat{VAR(\hat{\beta}_1)} = \frac{\sum(x_i - \bar{x})^2\hat{u}_i^2}{SST_x^2}$

  - $\hat{u}_i^2$ is just the squared residual associated with each observation $i$
  
- These standard errors have LOTS of different names:

  - "White standard errors"
  
  - "Huber-White standard errors"
  
  - "Robust standard errors"
  
  - "Heteroskedasticity-robust standard errors"
  

---

# Estimating Error Variance

- So we have $VAR(\hat{\beta}_1) = \frac{\sigma^2}{SST_x}$

  - This is a **conceptual** quantity
  
- How do we actually calculate it?

  - Recall from the univariate case where we wrote $VAR(\bar{Y}) = \frac{\sigma^2}{n}$
  
  - We said it is rare that we actually know $\sigma^2$, but we still estimate it with $S_u^2 = \frac{\sum(y_i - \bar{y})^2}{n-1}$
  
- Here, we do something very similar: $\hat{\sigma}^2 = \frac{\sum(u_i)^2}{n-2} = \frac{SSR}{n-2}$

  - Why $n-2$?
  
---

# Estimating Error Variance

- Thus we plug this into our two formulas for $VAR(\hat{\beta}_0)$ and $VAR(\hat{\beta}_1)$

$$
\begin{aligned}
\widehat{VAR(\hat{\beta}_0)} &= \frac{\hat{\sigma}^2 \frac{\sum x_i^2}{n}}{SST_x}\\
&= \frac{\frac{SSR}{n-2}\frac{\sum x_i^2}{n}}{SST_x}\\
\widehat{VAR(\hat{\beta}_1)} &= \frac{\hat{\sigma}^2}{SST_x}\\
&= \frac{\frac{SSR}{n-2}}{SST_x}\\
\end{aligned}
$$

---

# Estimating Error Variance

- As we discussed in the theoretical case, $\hat{\sigma}^2$ is a very interesting quantity, because $\sqrt{\hat{\sigma}^2} = \hat{\sigma} \overset{p}\rightarrow \sigma$

  - $\hat{\sigma}$ is expressed in units of $y$
  
  - Tell us how far the typical fitted value of $y$ is from the observed value
  
  - Theoretically, the extent to which unexplained factors are affecting the value of $y$
  
  - **VERY INFORMATIVE STATISTIC THAT NO ONE REALLY PAYS ATTENTION TO**
  
- Terms for $\hat{\sigma}$:

  - Wooldridge: "standard error of regression" (SER)
  
  - Root MSE or RMSE
  
  - Standard error of the estimate (SEE)
  
  - `R`: Residual standard error



---

# Hypothesis Testing

- Remember all these fun times we had?

```{r,echo=F,warning=F,message=F}
# Load necessary library
library(ggplot2)

# Define the mean and standard deviation
mean <- 0
sd <- 1

# Define the range for x values
x <- seq(-4, 4, length.out = 1000)

# Define the density function for the normal distribution
y <- dnorm(x, mean, sd)

# Calculate the critical value for a 95% confidence interval
z <- qnorm(c(0.025, 0.975))

ggplot(data.frame(x, y), aes(x, y)) +
  geom_ribbon(data = subset(data.frame(x, y), x > -Inf & x < Inf),
              aes(ymin = 0, ymax = y),
              fill = "gray40",
              alpha = 0.5,color = 'black') +
  geom_ribbon(data = subset(data.frame(x, y), x > z[1] & x < z[2]),
              aes(ymin = 0, ymax = y),
              fill = "white",
              alpha = 1,color = 'black') +
  labs(title = bquote("Sampling Distribution of"~ hat(theta)),
       x = bquote(z),
       y = bquote(f(hat(theta))==phi)) +
  theme_minimal() + 
  geom_vline(xintercept = 0,linetype = 'dashed') + 
  annotate(geom = 'segment',x = 1.96,y = -.05,xend = Inf,yend = -.05,
             size = 1.6,color = 'gray40') +
  annotate(geom = 'segment',x = -1.96,y = -.05,xend = -Inf,yend = -.05,
             size = 1.6,color = 'gray40') +
  annotate(geom = 'segment',x = -Inf,y = -.05,xend = Inf,yend = -.05,
           arrow = arrow(length=unit(0.30,"cm"), ends="both", type = "closed")) +
  annotate(geom = 'segment',x = 1.96,y = -.075,xend = 1.96,yend = -0.025) + 
  annotate(geom = 'segment',x = -1.96,y = -.075,xend = -1.96,yend = -0.025) + 
  annotate(geom = 'label',x = 0,y = -Inf,label = expression(theta[0]),vjust = 0,parse = T) + 
  # annotate(geom = 'segment',x = -.95,xend = .95,y = .25,yend= .25,
  #          arrow = arrow(length=unit(0.30,"cm"), ends="both", type = "closed")) + 
  annotate(geom = 'label',x = 0,y = Inf,label = expression(1-alpha),vjust = 5.5,parse = T) + 
  annotate(geom = 'label',x = -2.5,y = .01,label = expression(alpha/2),vjust = 0,parse = T) + 
  annotate(geom = 'label',x = 2.5,y = .01,label = expression(alpha/2),vjust = 0,parse = T) + 
  scale_x_continuous(breaks = c(-1.96,1.96),labels = c(expression(theta[0]-z[alpha/2]~sigma[hat(theta)]),expression(theta[0]+z[alpha/2]~sigma[hat(theta)]))) + 
  theme(axis.title.x = element_text(hjust = 1))
```

---

# Hypothesis Testing

- We now have the tools to do this with $\hat{\beta}_1$! (And $\hat{\beta}_0$, although that is rarely the quantity of interest.)

- Note that we typically are interested in whether $\hat{\beta}_1$ is zero:

  - Null $H_0$: $\beta_1 = 0$
  
  - Alternative $H_A$: $\beta_1 \neq 0$
  
  - Test statistic: Critical $t$ value for Student's T-test for our estimator $\hat{\beta}_1$
  
  - Rejection Region: $\hat{\beta}_1 < 0 - t_{\alpha/2,\nu}*\sqrt{\widehat{VAR(\hat{\beta}_1)}}$ or $\hat{\beta}_1 > 0 + t_{\alpha/2,\nu}*\sqrt{\widehat{VAR(\hat{\beta}_1)}}$
  
---

# Hypothesis Testing

- What is $\sqrt{\widehat{VAR(\hat{\beta}_1)}}$? 

--

  - The **standard error** of the estimator $\hat{\beta}_1$, or $\widehat{se(\hat{\beta}_1)}$, (or often just $se_{\hat{\beta}_1}$)
  
- What is $\nu$?

--

  - The **degrees of freedom**: This will be $n - k - 1$. $n$ observations minus $k$ parameters (in this case just one: $\hat{\beta}_1$) - 1 (for the intercept $\hat{\beta}_0$)

---

# Hypothesis Testing


```{r,echo=F,warning=F,message=F}
# Load necessary library
library(ggplot2)

# Define the mean and standard deviation
mean <- 0
sd <- 1

# Define the range for x values
x <- seq(-4, 4, length.out = 1000)

# Define the density function for the normal distribution
y <- dnorm(x, mean, sd)

# Calculate the critical value for a 95% confidence interval
z <- qnorm(c(0.025, 0.975))

ggplot(data.frame(x, y), aes(x, y)) +
  geom_ribbon(data = subset(data.frame(x, y), x > -Inf & x < Inf),
              aes(ymin = 0, ymax = y),
              fill = "gray40",
              alpha = 0.5,color = 'black') +
  geom_ribbon(data = subset(data.frame(x, y), x > z[1] & x < z[2]),
              aes(ymin = 0, ymax = y),
              fill = "white",
              alpha = 1,color = 'black') +
  labs(title = bquote("Sampling Distribution of"~ hat(beta)[1]),
       x = bquote(t),
       y = bquote(f(hat(theta))==phi)) +
  theme_minimal() + 
  geom_vline(xintercept = 0,linetype = 'dashed') + 
  annotate(geom = 'segment',x = 1.96,y = -.05,xend = Inf,yend = -.05,
             size = 1.6,color = 'gray40') +
  annotate(geom = 'segment',x = -1.96,y = -.05,xend = -Inf,yend = -.05,
             size = 1.6,color = 'gray40') +
  annotate(geom = 'segment',x = -Inf,y = -.05,xend = Inf,yend = -.05,
           arrow = arrow(length=unit(0.30,"cm"), ends="both", type = "closed")) +
  annotate(geom = 'segment',x = 1.96,y = -.075,xend = 1.96,yend = -0.025) + 
  annotate(geom = 'segment',x = -1.96,y = -.075,xend = -1.96,yend = -0.025) + 
  annotate(geom = 'label',x = 0,y = -Inf,label = expression(beta[1]==0),vjust = 0,parse = T) + 
  # annotate(geom = 'segment',x = -.95,xend = .95,y = .25,yend= .25,
  #          arrow = arrow(length=unit(0.30,"cm"), ends="both", type = "closed")) + 
  annotate(geom = 'label',x = 0,y = Inf,label = expression(1-alpha),vjust = 5.5,parse = T) + 
  annotate(geom = 'label',x = -2.5,y = .01,label = expression(alpha/2),vjust = 0,parse = T) + 
  annotate(geom = 'label',x = 2.5,y = .01,label = expression(alpha/2),vjust = 0,parse = T) + 
  scale_x_continuous(breaks = c(-1.96,1.96),labels = c(expression(0-t[alpha/2~nu]~se[hat(beta)[1]]),expression(0+t[alpha/2~nu]~se[hat(beta)[1]]))) + 
  theme(axis.title.x = element_text(hjust = 1))
```

---

```{r}
require(tidyverse)
set.seed(123)
n <- 100
X <- rnorm(n)
Y <- rnorm(n,mean = X)

summary(lm(Y~X))
```

---

# Manual Calculation!

```{r}
b1_hat <- cov(X,Y)/var(X)
b0_hat <- mean(Y) - (cov(X,Y)/var(X))*mean(X)

preds <- b0_hat + b1_hat*X
resids <- Y - preds
mean(resids)
```

---

# Manual Calculation!

```{r}
SSR <- sum(resids^2)
sigma2_hat <- SSR/(n-2)
sigma_hat <- sqrt(sigma2_hat)

SST_x <- sum((X - mean(X))^2)
S_xx <- sum(X^2) - n*mean(X)^2 # Equivalent ways

VAR0_hat <- (sigma2_hat*(sum(X^2)/n))/SST_x
se0_hat <- sqrt(VAR0_hat)

VAR1_hat <- sigma2_hat/SST_x
se1_hat <- sqrt(VAR1_hat)
```

---

# Manual Calculation!

```{r}
cat(c(b0_hat,se0_hat),'\n',c(b1_hat,se1_hat))

summary(lm(Y~X))
```

---

# Other output

- Regression output includes two additional columns

  - `t value` and `Pr(>|t|)`
  
- `t value` is just $\frac{\hat{\beta}_1}{\widehat{se(\hat{\beta}_1)}}$ (or $\frac{\hat{\beta}_0}{\widehat{se(\hat{\beta}_0)}}$)

  - `b1_hat / se1_hat = ` `r b1_hat / se1_hat`
  
- `Pr(>|t|)` is literally the probably of observing a value as large as the absolute value of the t-value

  - I.e., the $p$**-value**!: "attained significance level" or "smallest level of $\alpha$ for which we would **reject** $H_0$

---

# Controlling for a variable

- We talked about OVB (**O**mitted **V**ariable **B**ias)

  - Subset of broader conversation about bias
  
- We might want to "control" for $z$ to remove OVB (preventing the $\beta_2 z_i + \nu_i$ from being "buried in the $u$)

  - But remember that failing to control for $z$ is only a problem **if** both (1) $\beta_2 \neq 0$ and (2) $cov(z,x) \neq 0$
  
- *Ceteris paribus*: "all things being equal"
  
  - Want to estimate a *ceteris paribus* relationship between $X$ and $Y$
  
  - What would the relationship look like if all other aspects of our units were the same?
  
  - Commonly invoked for causal claims, but more on that next semester
  
---

# Controlling for a variable

- Let's get precise with our terminology:

  - $Z$ is a potential **confound**
  
  - If $Z$ "confounds" the relationship between $X$ and $Y$, it **renders the relationship spurious**

- How about some examples? 

--

| $X$ | $Y$ | $Z$ | 
| ------------ | ------------------- | -------------------- |
| College degree | Salary at age 25 | Ability |
| Female | Pro-Choice | Democrat |
| First-born | IQ Score | Parental involvement | 
| Asian-American | Trump Support | Vietnamese | 
| Own a home | Participated in Women's March | Year of Birth |
| ... | ... | ... |

---

# Controlling for a variable

- To determine whether $Z$ renders the relationship between $X$ and $Y$ spurious, we...

  - "control for $Z$"
  
  - "condition on $Z$"
  
  - "hold $Z$ constant"
  
- These all mean the same thing conceptually, but there are several different ways to do this

- Ideally, we would do exactly what "holding $Z$ constant" suggests: divide our units by categories of $Z$ and examine the relationship between $X$ and $Y$ within each category of $Z$

  - I.e., if women are more pro-choice, we want to see this among *both* Democrats and Republicans
  
  - If the relationship persists after holding $Z$ constant, we say it is not spurious
  
  - If it no longer holds, we say that $Z$ is a confound rendering the relationship between $X$ and $Y$ spurious
  
- In practice, we usually do something much less careful

---

# Controlling for a variable

- We often will make our assumptions explicit with a **D**irected **A**cyclic **G**raph (DAG)

  - This encodes our intuition about what the population parameters of interest might be

```{r test,echo=FALSE,message=F,warning=F}
# library(DiagrammeR)
# m <- mermaid("graph LR;
# 		   X-->Y;
# 		   Z-->X;
# 		   Z-->Y
# 		   ")
# m
# widgetframe::frameWidget(m)

require(ggdag)
simple_dag_with_coords <- dagify(
  Y ~ X + Z,
  X ~ Z,
  # exposure = "x",
  # outcome = "y",
  coords = list(x = c(X = 1, Z = 2, Y = 3),
                y = c(X = 2, Z = 3, Y = 2))
)

ggdag(simple_dag_with_coords,text_size = 5) +
  theme_dag()
```

---

# Controlling for a variable

- Let's tackle a classic: education and income

  - $Y$: income
  
  - $X$: education
  
- What is $Z$?

--

  - Parent's education?

---

# Controlling for a variable

- Start by looking at all three relationships separately

```{r,message=F,warning=F}
require(tidyverse)
require(haven) # To open .dta files

# dat <- read_dta('../NFGH/GSS_stata/GSS_stata/gss7222_r1.dta')
# 
# dat <- dat %>%
#   select(realinc,educ,paeduc) %>%
#   sample_n(size = 10000,replace = F) %>%
#   drop_na()
# 
# gc()
# 
# write_rds(dat,file = './Data/gss7222_r1_sample.rds')

dat <- read_rds('https://github.com/jbisbee1/PSCI_8356_F2024/raw/refs/heads/main/Data/gss7222_r1_sample.rds')
```

- (`gc()` helps save memory after I dropped thousands of rows and columns)

---

# Controlling for a variable

- Start by looking at all three relationships separately

```{r,message=F,warning=F}
dat %>%
  group_by(educ) %>%
  summarise(income = mean(realinc,na.rm=T))
```

---

# Controlling for a variable

- Start by looking at all three relationships separately

```{r,message=F,warning=F}
dat %>%
  ggplot(aes(x = educ,y = realinc)) + 
  geom_point()
```

---

# Controlling for a variable

- Start by looking at all three relationships separately

```{r,message=F,warning=F,echo=F}
dat %>%
  ggplot(aes(x = educ,y = realinc)) + 
  geom_jitter() + 
  geom_smooth(aes(color = 'Local linear smoother')) + 
  geom_smooth(aes(color = 'Linear regression'),method = 'lm',linetype = 'dashed') + 
  scale_color_manual(name = 'Method',values = c('darkblue','red'))
```

---

# Controlling for a variable

- Start by looking at all three relationships separately

```{r,message=F,warning=F,echo=F}
dat %>%
  ggplot(aes(x = paeduc,y = realinc)) + 
  geom_jitter() + 
  geom_smooth(aes(color = 'Local linear smoother')) + 
  geom_smooth(aes(color = 'Linear regression'),method = 'lm',linetype = 'dashed') + 
  scale_color_manual(name = 'Method',values = c('darkblue','red'))
```

---

# Controlling for a variable

- Start by looking at all three relationships separately

```{r,message=F,warning=F,echo=F}
dat %>%
  ggplot(aes(x = paeduc,y = educ)) + 
  geom_jitter() + 
  geom_smooth(aes(color = 'Local linear smoother')) + 
  geom_smooth(aes(color = 'Linear regression'),method = 'lm',linetype = 'dashed') + 
  scale_color_manual(name = 'Method',values = c('darkblue','red'))
```

---

# Controlling for a variable

- Clearly evidence of:

  - $\beta_1 \neq 0$
  
  - $\beta_2 \neq 0$
  
  - $cov(educ,paeduc) \neq 0$
  
- I.e., OVB!

- Think through what this will mean for the following regression: $realinc_{i} = \beta_0 + \beta_1 educ_{i} + u_{i}$

---

# Controlling for a variable

- Let's "control" for parent's education in three ways

1. Fewest Assumptions: Just visualize it with a local linear smoother, and subset to different values of parent's education

  - No linearity assumption between $X$ and $Y$
  
  - Different relationship between $X$ and $Y$ for different values of $Z$

2. More Assumptions: Run multiple linear regressions, subsetting to different values of parent's education

  - Linearity assumption between $X$ and $Y$
  
  - Different (linear) relationships between $X$ and $Y$ for different values of $Z$
  
3. Most Assumptions: Run single linear regression, adding $Z$ as an additional predictor

  - Linearity assumption between $X$ and $Y$
  
  - Same linear relationship between $X$ and $Y$ for all values of $Z$


---

# Controlling for a variable

- First, let's take `paeduc` and transform it into a categorical measure

```{r}
dat <- dat %>%
  mutate(paeduc_cat = factor(ifelse(paeduc < 9,'8th gr or less',
                             ifelse(paeduc < 12,'HS dropout',
                                    ifelse(paeduc == 12,'HS degree',
                                           ifelse(paeduc < 16,'Some college',
                                                  ifelse(paeduc == 16,'College degree','Post grad'))))),
                             levels = c('8th gr or less','HS dropout','HS degree','Some college','College degree','Post grad')))
dat %>% count(paeduc_cat)
```
---

# Controlling for a variable: Fewest assumptions

```{r,message=F,warning=F}
p <- dat %>%
  ggplot(aes(x = educ,y = realinc)) + 
  geom_point() + 
  geom_smooth() + 
  facet_wrap(~paeduc_cat)
```

---

# Controlling for a variable: Fewest assumptions

```{r,message=F,warning=F}
p
```

---

# Controlling for a variable: More assumptions

```{r}
res <- list()
for(i in unique(dat$paeduc_cat)) {
  res[[i]] <- lm(realinc ~ educ,dat %>% 
                               filter(paeduc_cat == i))
}

# 8th grade or less
summary(res$`8th gr or less`)$coefficients

# High school dropout
summary(res$`HS dropout`)$coefficients
```

---

# Controlling for a variable: More assumptions

```{r}
summary(res$`HS degree`)$coefficients
summary(res$`Some college`)$coefficients
summary(res$`College degree`)$coefficients
summary(res$`Post grad`)$coefficients
```

---

# Controlling for a variable: More assumptions

```{r,echo=F,warning=F,message=F}
toplot <- NULL
for(i in names(res)) {
  toplot <- toplot %>%
    bind_rows(summary(res[[i]])$coefficients %>%
    data.frame() %>%
      slice(2) %>%
    mutate(parent_education = i) %>%
    as_tibble())
}

toplot %>%
  mutate(parent_education = factor(parent_education,levels = c('8th gr or less','HS dropout','HS degree','Some college','College degree','Post grad'))) %>%
  ggplot(aes(x = Estimate,y = parent_education)) + 
  geom_point() + 
  geom_errorbarh(aes(xmin = Estimate - 1.96*`Std..Error`,xmax = Estimate + 1.96*`Std..Error`))
```

---

# Controlling for a variable: Most assumptions

```{r}
summary(lm(realinc ~ educ + paeduc,dat))
```

---

# FWL and Partialling Out

- To understand what multiple regression looks like in matrix form, we need some helper concepts

- The "residual maker" is a matrix $\mathbf{M}$ that, when multiplied by $\mathbf{y}$, creates **residuals** $\mathbf{u}$

- Start with the definition of the residual: $\mathbf{u} = \mathbf{y} - \hat{\mathbf{y}}$ and substitute $\hat{\mathbf{y}} = \mathbf{X}\hat{\beta}$ in

$$
\begin{aligned}
\mathbf{u} &= \mathbf{y} - \mathbf{X}\hat{\beta}
\end{aligned}
$$
- Now replace $\hat{\beta}$ with the definition $(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{y}$

$$
\begin{aligned}
\mathbf{u} &= \mathbf{y} - \mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{y}\\
&= (\mathbf{I} - \mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top)\mathbf{y}\\
&= \mathbf{M}\mathbf{y}
\end{aligned}
$$

---

# FWL and Partialling Out

- $\mathbf{M}$ is super helpful. It is both square and **idempotent**, meaning that $\mathbf{M}\mathbf{M} = \mathbf{M}$. (Try proving this for yourself!)

- It also has the properties:

  1. $\mathbf{M}\mathbf{X} = \mathbf{0}$
  
  2. $\mathbf{M}\mathbf{u} = \mathbf{u}$

---

# FWL and Partialling Out

- The "hat" matrix is a matrix $\mathbf{H}$ that, when multiplied by $\mathbf{y}$, creates **predicted values** $\mathbf{\hat{y}}$

$$
\begin{aligned}
\hat{\mathbf{y}} &= \mathbf{y} - \mathbf{u} \\
&= (\mathbf{I} - \mathbf{M})\mathbf{y} \\
&= \mathbf{H}\mathbf{y}
\end{aligned}
$$
- So we now have $\mathbf{M} = \mathbf{I} - \mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top$ and $\mathbf{H} = \mathbf{I} - \mathbf{M}$

- But this just means $\mathbf{H} = \mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top$

---

# FWL and Partialling Out

- These "hat" and "residual maker" matrices can help us understand OVB and, more generally, what "controlling" for a variable means in the matrix world

- Consider the classic example where the true regression equation is given by $\mathbf{y} = \mathbf{x}_1\beta_1 + \mathbf{x}_2\beta_2 + u$, but we mistakenly omit $\mathbf{x}_2$

- The true normal equation is:

$$
\begin{aligned}
\left[ {\begin{array}{cc}
\mathbf{x}^\top_1\mathbf{x}_1 & \mathbf{x}^\top_1\mathbf{x}_2 \\
\mathbf{x}^\top_2\mathbf{x}_1 & \mathbf{x}^\top_2\mathbf{x}_2 
  \end{array} } \right]^{-1}\cdot 
  \left[ {\begin{array}{c}
\mathbf{x}_1\mathbf{y} \\
\mathbf{x}_2\mathbf{y}
  \end{array} } \right] &= 
  \left[ {\begin{array}{c}
\hat{\beta}_1 \\
\hat{\beta}_2
  \end{array} } \right]
\end{aligned}
$$

---

# FWL and Partialling Out

- First, solve for $\hat{\beta}_1$

$$
\begin{aligned}
(\mathbf{x}_1^\top\mathbf{x}_1)\hat{\beta}_1 + (\mathbf{x}_1^\top\mathbf{x}_2)\hat{\beta}_2 &= \mathbf{x}_1^\top\mathbf{y}\\
(\mathbf{x}_1^\top\mathbf{x}_1)\hat{\beta}_1 &= \mathbf{x}_1^\top\mathbf{y} -  (\mathbf{x}_1^\top\mathbf{x}_2)\hat{\beta}_2\\
\hat{\beta}_1 &= (\mathbf{x}_1^\top\mathbf{x}_1)^{-1}\mathbf{x}_1^\top\mathbf{y} - (\mathbf{x}_1^\top\mathbf{x}_1)^{-1}(\mathbf{x}_1^\top\mathbf{x}_2)\hat{\beta}_2\\
\hat{\beta}_1 &= (\mathbf{x}_1^\top\mathbf{x}_1)^{-1}\mathbf{x}_1^\top(\mathbf{y} - \mathbf{x}_2\hat{\beta}_2)
\end{aligned}
$$

- Recognize this? 

--

  - $(\mathbf{x}^\top_1\mathbf{x}_1)^{-1}\mathbf{x}^\top_1\mathbf{x}_2$ is the regression of $\mathbf{x}_2$ on $\mathbf{x}_1$. This will be zero if $\mathbf{x}_2$ is unrelated to $\mathbf{x}_1$
  
  - $\hat{\beta}_2$ is the relationship between $\mathbf{y}$ and $\mathbf{x}_2$.
  
--

- **This is just OVB in matrix form**

---

# FWL and Partialling Out

- Now let's see what happens when we control for $\mathbf{x}_2$

- Start with $\hat{\beta}_1 = (\mathbf{x}^\top_1\mathbf{x}_1)^{-1}\mathbf{x}^\top_1(\mathbf{y} - \mathbf{x}_2\hat{\beta}_2)$

- Then do direct multiplication on the second row in 

$$
\begin{aligned}
\left[ {\begin{array}{cc}
\mathbf{x}^\top_1\mathbf{x}_1 & \mathbf{x}^\top_1\mathbf{x}_2 \\
\mathbf{x}^\top_2\mathbf{x}_1 & \mathbf{x}^\top_2\mathbf{x}_2 
  \end{array} } \right]^{-1}\cdot 
  \left[ {\begin{array}{c}
\mathbf{x}_1\mathbf{y} \\
\mathbf{x}_2\mathbf{y}
  \end{array} } \right] &= 
  \left[ {\begin{array}{c}
\hat{\beta}_1 \\
\hat{\beta}_2
  \end{array} } \right]
\end{aligned}
$$
- to yield $\mathbf{x}_2^\top\mathbf{x}_1\hat{\beta}_1 + \mathbf{x}_2^\top\mathbf{x}_2\hat{\beta}_2 = \mathbf{x}_2^\top\mathbf{y}$

- Finally, substitute in our definition of $\hat{\beta}_1$ to get $\mathbf{x}_2^\top\mathbf{x}_1(\mathbf{x}_1^\top\mathbf{x}_1)^{-1}\mathbf{x}_1^\top\mathbf{y} - \mathbf{x}_2^\top\mathbf{x}_1(\mathbf{x}_1^\top\mathbf{x}_1)^{-1}\mathbf{x}_1^\top\mathbf{x}_2\hat{\beta}_2 + \mathbf{x}_2^\top\mathbf{x}_2\hat{\beta}_2 = \mathbf{x}_2^\top\mathbf{y}$

---

# FWL and Partialling Out

- So this is horrible, but try this!

$$
\begin{aligned}
\mathbf{x}_2^\top\mathbf{y} - \mathbf{x}_2^\top\mathbf{x}_1(\mathbf{x}_1^\top\mathbf{x}_1)^{-1}\mathbf{x}_1^\top\mathbf{y} &= \mathbf{x}_2^\top\mathbf{x}_2\hat{\beta}_2 - \mathbf{x}_2^\top\mathbf{x}_1(\mathbf{x}_1^\top\mathbf{x}_1)^{-1}\mathbf{x}_1^\top\mathbf{x}_2\hat{\beta}_2\\
\mathbf{x}_2^\top\mathbf{y} - \mathbf{x}_2^\top\mathbf{x}_1(\mathbf{x}_1^\top\mathbf{x}_1)^{-1}\mathbf{x}_1^\top\mathbf{y} &= [\mathbf{x}_2^\top\mathbf{x}_2 - \mathbf{x}_2^\top\mathbf{x}_1(\mathbf{x}_1^\top\mathbf{x}_1)^{-1}\mathbf{x}_1^\top\mathbf{x}_2]\hat{\beta}_2\\
\mathbf{x}_2^\top\mathbf{y} - \mathbf{x}_2^\top\mathbf{x}_1(\mathbf{x}_1^\top\mathbf{x}_1)^{-1}\mathbf{x}_1^\top\mathbf{y} &= [(\mathbf{x}_2^\top - \mathbf{x}_2^\top\mathbf{x}_1(\mathbf{x}_1^\top\mathbf{x}_1)^{-1}\mathbf{x}_1^\top)\mathbf{x}_2]\hat{\beta}_2\\
\mathbf{x}_2^\top\mathbf{y} - \mathbf{x}_2^\top\mathbf{x}_1(\mathbf{x}_1^\top\mathbf{x}_1)^{-1}\mathbf{x}_1^\top\mathbf{y} &= [(\mathbf{x}_2^\top(\mathbf{I} - \mathbf{x}_1(\mathbf{x}_1^\top\mathbf{x}_1)^{-1}\mathbf{x}_1^\top)\mathbf{x}_2]\hat{\beta}_2\\
(\mathbf{x}_2^\top - \mathbf{x}_2^\top\mathbf{x}_1(\mathbf{x}_1^\top\mathbf{x}_1)^{-1}\mathbf{x}_1^\top)\mathbf{y} &= [(\mathbf{x}_2^\top(\mathbf{I} - \mathbf{x}_1(\mathbf{x}_1^\top\mathbf{x}_1)^{-1}\mathbf{x}_1^\top)\mathbf{x}_2]\hat{\beta}_2\\
\mathbf{x}_2^\top(\mathbf{I} - \mathbf{x}_1(\mathbf{x}_1^\top\mathbf{x}_1)^{-1}\mathbf{x}_1^\top)\mathbf{y} &= [(\mathbf{x}_2^\top(\mathbf{I} - \mathbf{x}_1(\mathbf{x}_1^\top\mathbf{x}_1)^{-1}\mathbf{x}_1^\top)\mathbf{x}_2]\hat{\beta}_2\\
\hat{\beta}_2 &= [(\mathbf{x}_2^\top(\mathbf{I} - \mathbf{x}_1(\mathbf{x}_1^\top\mathbf{x}_1)^{-1}\mathbf{x}_1^\top)\mathbf{x}_2]^{-1}\mathbf{x}_2^\top(\mathbf{I} - \mathbf{x}_1(\mathbf{x}_1^\top\mathbf{x}_1)^{-1}\mathbf{x}_1^\top)\mathbf{y}\\
&= (\mathbf{x}_2^\top\mathbf{M}_1\mathbf{x}_2)^{-1}(\mathbf{x}_2^\top\mathbf{M}_1\mathbf{y})
\end{aligned}
$$

---

# FWL and Partialling Out

- So we now have $\hat{\beta}_2 = (\mathbf{x}_2^\top\mathbf{M}_1\mathbf{x}_2)^{-1}(\mathbf{x}_2^\top\mathbf{M}_1\mathbf{y})$

- Remember that $\mathbf{M}$ is the residual maker, meaning that $\mathbf{M}_1$ is making residuals for regressions on the $\mathbf{x}_1$ variables

--

  - $\mathbf{M}_1\mathbf{y}$ therefore creates residuals from regressing $\mathbf{y}$ on $\mathbf{x}_1$
  
  - $\mathbf{M}_1\mathbf{x}_2$ therefore creates residuals from regressing $\mathbf{x}_2$ on $\mathbf{x}_1$
  
--

- Since $\mathbf{M}$ is both idempotent and symmetric, we can rewrite as $\hat{\beta}_2 = (\mathbf{x}^{*\top}_2\mathbf{x}_2)^{-1}\mathbf{x}^{*\top}_2\mathbf{y}^*$

  - Where $\mathbf{x}_2^* = \mathbf{M}_1\mathbf{x}_2$ and $\mathbf{y}^* = \mathbf{M}_1\mathbf{y}$
  
--

- This leads to the **Frisch-Waugh-Lovell** Theorem: In the OLS regression of a vector $\mathbf{y}$ on two sets of variables $\mathbf{x}_1$ and $\mathbf{x}_2$, $\hat{\beta}_2$ is the coefficient obtained when the residuals from a regression of $\mathbf{y}$ on $\mathbf{x}_1$ alone are regressed on the set of residuals obtained when $\mathbf{x}_2$ is regressed on $\mathbf{x}_1$

---

# FWL and Partialling Out

- Imagine the following model (reverting back to the layperson's notation here): $Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + u$

- According to FWL:

  1. Regress $Y$ on $X_1$ and obtain the residuals $\hat{u}_1$ (i.e., $\mathbf{M}_1\mathbf{y}$ in matrix notation)
  
  2. Regress $X_2$ on $X_1$ and obtain the residuals $\hat{u}_2$ (i.e., $\mathbf{M}_1\mathbf{x}_2$ in matrix notation)
  
  3. Regress $\hat{u}_1$ on $\hat{u}_2$: $\hat{u}_1 = \rho_0 + \rho_1 \hat{u}_2 + \epsilon$
  
--

- $\hat{\beta}_2$ will be equal to $\hat{\rho}_1$!

--

- Step 2 is called "partialling out" or "netting out" the effect of $X_1$. For this reason, the coefficients in multiple regression are often referred to as "partial regression coefficients". 

---

# FWL and Partialling Out

- Let's try it!

```{r}
X1 <- rnorm(100)
X2 <- rnorm(100)

# True beta_1 = 1, beta_2 = -1, beta_3 = 3
Y <- X1 - X2 + rnorm(100) 

# Multiple regression
mFull <- lm(Y ~ X1 + X2)

# FWL way
u_1 <- resid(lm(Y ~ X1))
u_2 <- resid(lm(X2 ~ X1))
mRes <- lm(u_1 ~ u_2)
```

---

# FWL and Partialling Out

- As promised, we get the same estimates for $\hat{\beta}_2$ whether we estimate them in the standard multiple regression setting, or if we use the FWL residualizer approach

```{r}
# Same coefficients!
round(coef(mFull)[c(3)],4)
round(coef(mRes)[c(2)],4)
```


---


---

# FWL and Partialling Out

- Imagine the following model (reverting back to the layperson's notation here): $Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + u$

- According to FWL:

  1. Regress $Y$ on $X_1$ and obtain the residuals $\hat{u}_1$ (i.e., $\mathbf{M}_1\mathbf{y}$ in matrix notation)
  
  2. Regress $X_2$ on $X_1$ and obtain the residuals $\hat{u}_2$ (i.e., $\mathbf{M}_1\mathbf{x}_2$ in matrix notation)
  
  3. Regress $X_3$ on $X_1$ and obtain the residuals $\hat{u}_3$ (i.e., $\mathbf{M}_1\mathbf{x}_3$ in matrix notation)
  
  4. Regress $\hat{u}_1$ on $\hat{u}_2$ and $\hat{u}_3$: $\hat{u}_1 = \rho_0 + \rho_1 \hat{u}_2 + \rho_2 \hat{u}_3 + \epsilon$
  
--

- $\hat{\beta}_2$ will be equal to $\hat{\rho}_1$ and $\hat{\beta}_3$ will be equal to $\hat{\rho}_3$!

--

- Steps 2 and 3 are called "partialling out" or "netting out" the effect of $X_1$. For this reason, the coefficients in multiple regression are often referred to as "partial regression coefficients". 

---

# FWL and Partialling Out

- Let's try it!

```{r}
X1 <- rnorm(100)
X2 <- rnorm(100)
X3 <- rnorm(100)

# True beta_1 = 1, beta_2 = -1, beta_3 = 3
Y <- X1 - X2 + 3*X3 + rnorm(100) 

# Multiple regression
mFull <- lm(Y ~ X1 + X2 + X3)

# FWL way
u_1 <- resid(lm(Y ~ X1))
u_2 <- resid(lm(X2 ~ X1))
u_3 <- resid(lm(X3 ~ X1))
mRes <- lm(u_1 ~ u_2 + u_3)
```

---

# FWL and Partialling Out

- As promised, we get the same estimates for $\hat{\beta}_2$ and $\hat{\beta}_3$ whether we estimate them in the standard multiple regression setting, or if we use the FWL residualizer approach

```{r}
# Same coefficients!
round(coef(mFull)[c(3,4)],4)
round(coef(mRes)[c(2,3)],4)
```


---

# Regression Diagnostics

- We have so many assumptions at this point! (How many can you list?)

--

  1. Linearity
  
  2. I.i.d. random sample
  
  3. Non-zero variance / no multicolinearity
  
  4. Zero-conditional mean
  
  5. Homoskedasticity / spherical errors
  
  6. Normally distributed errors (small samples)
  
--

- How can we be confident in these assumptions? **Diagnostics** (to an extent)

---

# Regression Diagnostics

- Running example of two DGPs

--

- DGP 1:

$$
\begin{aligned}
\textit{Income}_i &= 15 + 6*\textit{Labor}_i + 40*\textit{PhD}_i + u_i
\end{aligned}
$$

where $u \sim \mathcal{N}(0,5)$ and i.i.d. holds

--

- DGP 2:

$$
\begin{aligned}
\textit{Income}_i &= 15 + 6*\textit{Labor}_i + 40*\textit{PhD}_i + \textit{Labor}_i^2 + u_i
\end{aligned}
$$
where $u_i = 0.5*\textit{Labor}_i*e_i$,  $e_i \sim \mathcal{N}(0,5)$ and i.i.d. holds

--

- Note that our assumptions hold by construction in DGP 1, but not in DGP 2

--

  - Specifically, zero conditional mean holds only if *Labor* is mean zero
  
  - In addition, the errors are not homoskedastic
  
---

# Regression Diagnostics

- Estimate with $\textit{Income}_i = \beta_0 + \beta_1 \textit{Labor}_i + \beta_2 \textit{PhD}_i + u_i$

```{r,echo=F,warning=F}
# PROPS TO JML!
set.seed(12345)


################################################################
## This file aims to do three things:
## 1) Demonstrate what is meant by repeated sampling in the regression context
## 2) Make crystal clear what the assumptions required for inference say about
## the data generating process
## 3) Show how to use diagnostic plots to glean that something might be off
## about the assumptions.


################################################################
## First, let's make some data that perfectly comports with the assumptions
## required for inference in OLS


## To start, we need to conjure up our independent variables.  
## Let's make a dummy variable for holding a PhD or not, and an interval
## variable for years in the labor force.

phd <- c(rep(0, 75), rep(1, 75))
labor <- sample(c(1:30), size = 150, replace = TRUE)


## Now let's generate our dependent variable, income, as a linear function of 
## years in labor market and degree status.
## In other words, we're assuming we know the data generating process.
## Let's have our DGP meet the assumptions of inference in OLS, and
## let's use it to generate some data (by which we mean generate the DV, income)

## We'll need a normally-distributed error term with mean 0 and a constant SD
error <- rnorm(150, mean = 0, sd = 5)

## Let's assume the true parameters are:
beta0 <- 15
beta1 <- 6
beta2 <- 40

## So now, let's generate one sample of our DV:

inc <- beta0 + beta1*labor + beta2*phd + error

## This is the step that would be repeated under "repeated sampling."

## Now let's collect our IVs and this DV into a dataframe.

data <- as.data.frame(cbind(inc, labor, phd))

## So we've created our outcome variable as a linear function of labor and phd,
## with normally-distributed error cenetered on zero with constant variance (sd = 5)


## Ok, now let's see what we get when we regress income on labor and phd
## using these data
```

```{r}
m1 <- lm(inc ~ labor + phd, data = data)
## Did we reproduce the truth?
summary(m1)
```

---

# Regression Diagnostics

- Visualize the results

```{r,echo=F,warning=F,fig.width=16,fig.height=6}
## And let's plot the regression lines separately for PhD and not
plot(labor, inc)
abline(coef(m1)[1], coef(m1)[2], lty = 2)
abline(coef(m1)[1]+coef(m1)[3], coef(m1)[2], lty = 1)

legend("topleft", legend = c("PhD", "No PhD"), title = "Degree", lty = c(1,2), lwd = c(1,1), cex = 1)
```

---

# Regression Diagnostics

- What if we are in the DGP2 world?

```{r,echo=F,warning=F}
## Let's contrast this with a true DGP that violates some of the assumptions
## for inference in OLS:

## For comparison, let's make another dataset

## Let's use the same IVs
phd <- c(rep(0, 75), rep(1, 75))
labor <- sample(c(1:30), size = 150, replace = TRUE)

## But notice the differences in the error term we're going to assume:

error <- .5*labor * rnorm(150, mean = 0, sd = 5)

## Let's keep the same assumed true parameter values

beta0 <- 15
beta1 <- 6
beta2 <- 40

## And let's generate our DV in this way:

inc <- beta0 + beta1*labor + beta2*phd + (labor^2) +  error

data2 <- as.data.frame(cbind(inc, labor, phd))
```

```{r}
m2 <- lm(inc ~ labor + phd, data = data2)

summary(m2)
```

---

# Regression Diagnostics

```{r,echo=F,warning=F,fig.height=6,fig.width=16}
## And plot it:

par(mfrow = c(1,1))
plot(labor, inc)
abline(coef(m2)[1], coef(m2)[2], lty = 2)
abline(coef(m2)[1]+coef(m2)[3], coef(m2)[2], lty = 1)

legend("topleft", legend = c("PhD", "No PhD"), title = "Degree", lty = c(1,2), lwd = c(5,5), cex = 1)
# plot(m2)
```

---

# Regression Diagnostics

- We are doing a bad job because:

  1. Income is not a linear function of labor and degree
  
  2. Errors are not mean zero
  
  3. Errors are not homoskedastic
  
--

- We **know** all this because we simulated these data

- But in reality, we typically never know what the true DGP is...how can we be alerted to the fact something is wrong with our model?

---

# Regression Diagnostics

- We can **look** at our residuals in a number of ways that inform us about our model's fit

--

1. Residuals vs. Fitted Values: Tells us if the data are roughly linear (smoother is roughly a horizontal line) and if there is heteroskedasticity (residuals are larger for some observations than for others)

2. Normal Q-Q Plot: Compares quantiles of our observed residuals to quantiles of hypothetical residuals that are normally distributed. If points cling to the 45 degree line, the residuals are normally distributed.

3. Scale-Location Plot: Similar to Residuals vs. Fitted, except we put the y-axis is now the square root of the standardized residuals. Also informs us about heteroskedasticity (shouldn't see a pattern) and identifies potential outliers

4. Residuals vs.k Leverage: Visualizes the **influence** of each observation on the regression coefficients. Points that are far from other points, especially those that are close to the dashed red lines, are problematic.

--

- Let's look at each in turn (`R` will produce all four by default if you simply run `plot(m1)` on your regression model)

---

# Residuals vs. Fitted


```{r,echo=F,warning=F,fig.width=14,fig.height=7}
par(mfrow = c(1,2))
plot(m1,which = 1)
plot(m2,which = 1)
```

---

# Normal Q-Q Plot


```{r,echo=F,warning=F,fig.width=14,fig.height=7}
par(mfrow = c(1,2))
plot(m1,which = 2)
plot(m2,which = 2)
```

---

# Scale-Location


```{r,echo=F,warning=F,fig.width=14,fig.height=7}
par(mfrow = c(1,2))
plot(m1,which = 3)
plot(m2,which = 3)
```

---

# Residuals vs. Leverage


```{r,echo=F,warning=F,fig.width=14,fig.height=7}
par(mfrow = c(1,2))
plot(m1,which = 5)
plot(m2,which = 5)
```

---

# Regression Diagnostics

- Problematic points: points can be unusual, but not all unusual points are problems

--

- Consider three types of points:

--

  1. Outlier Point: an observation with a **large residual**
  
  2. Leverage Point: an observation with an extreme value for $x$
  
  3. Influential Point: an observation that changes the slope of the regression line
  
--

- Always good to look at **influential points** to ensure there isn't an error in the measurement

--

  - But NOT always necessary to blindly throw them out
  
  - Better to characterize how sensitive the results are to them

---

# Choosing Variables

- With all this in mind, **how do you choose your variables** and **specify your regression equation**?

--

- We know we want to specify the true relationships, but how do we do this in practice?

--

- Theory, *theory*, **theory** is essential and should come first

--

  - This can be formalized with models, or it can be described with intuition, but no amount of diagnostic plots can replace careful theorizing prior to analysis
  
--

- That being said, let's consider some additional tests

--

- One simple method is to compare two specifications, say one that includes $x_2$ as a control and another that doesn't

--

  - How can we compare these models?

---

# Goodness of Fit

- Recall the definition of the $R^2$ from the simple regression case

$$
\begin{aligned}
R^2 &= \frac{\sum(\hat{y}_i - \bar{y})^2}{\sum(y_i - \bar{y})^2}\\
&= \frac{SSE}{SST} \\
&= 1 - \frac{SSR}{SST}
\end{aligned}
$$
- where $SSE$ is the explained sum of squares and $SSR$ is the residual sum of squares

--

- The $R^2$ will never decrease as we add additional predictors

--

  - This is because the denominator doesn't change, but the numerator will either increase or stay the same with additional predictors
  
--

- This makes the $R^2$ a pretty terrible metric for comparing models!

---

# Goodness of Fit

- Instead, we typically use the adjusted $R$-square value:

$$
\begin{aligned}
R^2_{adj} &= 1 - \left[\frac{\frac{SSR}{(n-k-1)}}{\frac{SST}{(n-1)}}\right]\\
&= 1 - \frac{\frac{\hat{\sigma}^2_u}{(n-k-1)}}{\frac{SST}{(n-1)}}\\
\end{aligned}
$$
--

- By construction, this will only increase with a new predictor if that variable's $t$-statistic is greater than 1 in absolute value

--

$$
R^2_{adj} = 1 - \frac{(1-R^2)(n-1)}{(n-k-1)}
$$

--

- On your own, think about when the $R^2$ and $R^2_{adj}$ will be similar and different?

---

# Too many variables

- Define a variable (denoted $W$) as **irrelevant** if it has not partial effect on $y$ in the population: $\frac{\partial y}{\partial W} = 0$

--

- If we include $W$ in our model (aka "overspecifying the model") will not bias the estimates since it does not violate our assumptions 1 through 4

  - In other words, if the true model is $y = \beta_0 + \beta_1 x + u$ but we specify $y = \beta_0 + \beta_1 x + \beta_2 W + u$, all $\hat{\beta}$ will be unbiased
  
--

- However, we can still harm our model if $W$ is collinear with $x$

--

  - Recall that $Var(\hat{\beta}_j) = \frac{\sigma^2}{n*var(x_j)*(1 - R^2_j)}$ where $R^2_j$ is the $R$-squared obtained from regressing $x_j$ on all other independent variables in the model
  
  - If $W$ is correlated with $x$, then $Var(\hat{\beta}_1)$ will become inflated, meaning our model is less **efficient**
  
  - Put a different way, our statistical power decreases, increasing the likelihood of falsely accepting the null
  
---

# Too many variables

- You can assess this threat by calculating each $R^2_j$ yourself

--

  - Also can calculate the **variance inflation factor** (VIF): $VIF(\hat{\beta}_j) = \frac{1}{1 - R^2_j}$
  
  - Can rewrite $Var(\hat{\beta}_j) = \frac{\sigma^2}{n*var(x_j)}VIF(\hat{\beta}_j)$, which is where it gets it's name...the factor by which $Var(\hat{\beta}_j)$ is inflated due to the fact that $x_j$ is correlated with other $x$'s in the model
  
--

- However, we will sometimes *want* to include controls that are correlated with $y$ but are **not** correlated with $x$

--

  - Note that these are not necessary to recover unbiased estimates (remember the definition of OVB?)
  
--

- Why do we want to control for some $Z$ where $\frac{\partial x}{\partial Z} = 0$ but $\frac{\partial y}{\partial Z} \neq 0$?

--

  - It helps explain variation in $y$, meaning that $\sigma^2$ is lower, meaning $Var(\hat{\beta}_j)$ is also lower
  
  - In other words, it makes all our estimates more **efficient**

---

# Hypotheses about Parameters

- Thus far, we've always been implicitly interested in a single coefficient, or testing each one at a time

--

- But we might be interested in how two coefficients related to each other

  - For example, your book has the example where researchers are interested whether the effect on income of an additional year of education at a junior college is as much as the effect of an additional year of education at four-year university.
  
  - The idea here is that $jc$'s are lower status in the U.S. than universities, so maybe employers value these years of education less.
  
  - (A complementary hypothesis would be that a jc education may be of lower quality.) 
  
- The model assumed is $\log(wage) = \beta_{0}+\beta_{1}jc+\beta_{2}univ+\beta_{3}work+u$

---

# Hypotheses about Parameters

- If we are interested in whether there is a **difference** in returns to education from junior colleges and universities, what is the appropriate null hypothesis?

--

- $H_0: \beta_1 = \beta_2$

- And the alternative?

--

- $H_A: \beta_1 < \beta_2$

- We can re-write as:

$$
\begin{aligned}
H_0&: \beta_1 - \beta_2 = 0\\
H_A&: \beta_1 - \beta_2 < 0
\end{aligned}
$$

- Thus our quantity of interest is $\beta_1 - \beta_2$ and our test statistic is $\frac{\hat{\beta}_1 - \hat{\beta}_2}{se(\hat{\beta}_1 - \hat{\beta}_2)}$

---

# Hypotheses about Parameters

- What is $se(\hat{\beta}_1 - \hat{\beta}_2)$?

$$
\begin{aligned}
se(\hat{\beta}_1 - \hat{\beta}_2) &= \sqrt{var(\hat{\beta}_1 - \hat{\beta}_2)}~~~\text{and}\\
var(\hat{\beta}_1 - \hat{\beta}_2) &= var(\hat{\beta}_1) + var(\hat{\beta}_2) - 2cov(\hat{\beta}_1,\hat{\beta}_2) ~~\text{so}\\
se(\hat{\beta}_1 - \hat{\beta}_2) &= \sqrt{var(\hat{\beta}_1) + var(\hat{\beta}_2) - 2cov(\hat{\beta}_1,\hat{\beta}_2)}
\end{aligned}
$$
--

- We can just grab these values from the variance-covariance matrix of estimated betas

--

- Or we can do an even easier trick! Let's denote $\theta = \beta_1 - \beta_2$, meaning that $\beta_1 = \theta + \beta_2$

- Therefore:

$$
\begin{aligned}
\log (wage) &= \beta_{0}+\beta_{1}jc+\beta_{2}univ+\beta _{3}work+u\\
&= \beta_{0}+(\theta + \beta_2)jc+\beta_{2}univ+\beta _{3}work+u\\
&= \beta_{0}+\theta jc+\beta_{2}(univ+jc) + \beta _{3}work+u\\
\end{aligned}
$$
--

- So easy! Just create a new variable that is the sum of $univ$ and $jc$ and look at the coefficient on $jc$!

---

# Multiple Linear Restrictions


- What if we want to know if a **group** of predictors are **jointly** significant?

- Start by defining an **unrestricted** model as $UR: y = \beta_0 + \beta_1 x_1 + \dots + \beta_k x_k + u$

- Then take the group of variables we are interested in evaluating and move them to the end of the regression

--

  - I.e., if there are $q$ variables we want to test are jointly significant, denote these as $\beta_{k-q+1}, \beta_{k-q+2}$ etc.
  
  - Thus our null hypothesis is $H_0: \beta_{k-q+1} = \beta_{k-q+2} = \dots = \beta_k = 0$
  
--

- We can write a restricted model as $R: y = \beta_0 + \beta_1 x_1 + \dots + \beta_{k-q}x_{k-q} + u$

- To test, we use the $F$-statistic defined as $F \equiv \frac{(SSR_r - SSR_{ur})/q}{SSR_{ur}/ (n-k-1)}$

--

  - Note that, since $SSR_r \geq SSR_{ur}$, $F > 0$
  
--

- This is the ratio of two independent $\chi^2$ random variables, divided by their respective degrees of freedom

- We can therefore conduct hypothesis testing using this: if it is extremely unlikely that we would obtain the observed $F$-statistic by chance, we reject the null $H_0$