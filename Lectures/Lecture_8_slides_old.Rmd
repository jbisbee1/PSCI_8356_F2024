---
title: "Lecture 8"
subtitle: "Quantitative Political Science"
author: "Prof. Bisbee"
institute: "Vanderbilt University"
date: "Lecture Date: 2023/09/26\n Slides Updated: `r Sys.Date()`"
output:
  xaringan::moon_reader:
    # self_contained: true
    chakra: libs/remark-latest.min.js
    lib_dir: libs
    css:
      - default
      - css/lexis.css
      - css/lexis-fonts.css
    #seal: false
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"

---

```{css,echo = F}
.small .remark-code { /*Change made here*/
  font-size: 85% !important;
}
.tiny .remark-code { /*Change made here*/
  font-size: 50% !important;
}
```

```{r,include=F}
set.seed(123)
options(width=60)
knitr::opts_chunk$set(fig.align='center',fig.width=9,fig.height=5)
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
require(tidyverse)
```

# Agenda

1. Bias and intuition

2. Confidence Intervals

3. $\sigma^2$

---

# Bias

- Our gut tells us that $\bar{Y} \equiv \frac{1}{n}\sum_i^n (Y_i)$ is not biased while $\bar{Y}_B \equiv \frac{1}{n}\sum_i^n (Y_i + 1)$ is

  - $\bar{Y}$ is the mean of the sample, so intuition tells us it should be unbiased for $\mu$
  
- But the **intuitive** estimator is not always the **unbiased** estimator

--

- Consider $S^2 = \frac{\sum_i (Y_i - \bar{Y})^2}{n}$ as a potential estimator for the population variance $\sigma^2$

- Prove that it is biased by taking its expectation


---

# Bias

$$
\begin{aligned}
E[S^2] &= E\bigg[\frac{\sum_i (Y_i - \bar{Y})^2}{n}\bigg]\\
&= E\bigg[\frac{\sum_i (Y_i^2 - 2Y_i\bar{Y} + \bar{Y}^2)}{n}\bigg]\\
&= E\bigg[\frac{\sum_i Y_i^2 - \color{red}{2\bar{Y}\sum_iY_i} + \sum_i\bar{Y}^2}{n}\bigg]\\
\end{aligned}
$$
---

# Bias

- Note that the definition of $\bar{Y} \equiv \frac{1}{n}\sum_i Y_i$

- Multiply both sides by $n$ to yield $n\bar{Y} \equiv \sum_i Y_i$

- Thus:

$$
\begin{aligned}
E[S^2] &= E\bigg[\frac{\sum_i Y_i^2 - \color{red}{2\bar{Y}\sum_iY_i} + \sum_i\bar{Y}^2}{n}\bigg]\\
&= E\bigg[\frac{\sum_i Y_i^2 - \color{red}{2\bar{Y}n\bar{Y}} + \sum_i\bar{Y}^2}{n}\bigg]\\
&= E\bigg[\frac{\sum_i Y_i^2 - 2n\bar{Y}^2 + n\bar{Y}^2}{n}\bigg]\\
&= E\bigg[\frac{\sum_i Y_i^2 - n\bar{Y}^2}{n}\bigg]\\
\end{aligned}
$$

---

# Bias

$$
\begin{aligned}
E[S^2] &= E\bigg[\frac{\sum_i Y_i^2 - n\bar{Y}^2}{n}\bigg]\\
&= \frac{1}{n}E\bigg[\sum_i Y_i^2 - n\bar{Y}^2\bigg]\\
&= \frac{1}{n}\bigg(\sum_i E[Y_i^2] - nE[\bar{Y}^2]\bigg)\\
&= \frac{1}{n}\bigg(\sum_i (E[Y_i^2] \color{red}{- E[Y_i]^2 + E[Y_i]^2}) - nE[\bar{Y}^2]\color{red}{- E[\bar{Y}]^2 + E[\bar{Y}]^2}\bigg)\\
&= \frac{1}{n}\bigg(\sum_i (VAR(Y_i) + E[Y_i]^2) - nVAR(\bar{Y}) + E[\bar{Y}]^2\bigg)
\end{aligned}
$$

---

# Bias

$$
\begin{aligned}
E[S^2] &= \frac{1}{n}\bigg(\sum_i (\sigma^2 + \mu^2) - n(\frac{\sigma^2}{n} + \mu^2)\bigg)\\
&= \frac{1}{n}\bigg(n(\sigma^2 + \mu^2) - \sigma^2 - n\mu^2\bigg)\\
&= \frac{1}{n}\bigg(n\sigma^2 + n\mu^2 - \sigma^2 - n\mu^2\bigg)\\
&= \frac{1}{n}\bigg(n\sigma^2 - \sigma^2\bigg)\\
&= \frac{n-1}{n}\sigma^2 \neq \sigma^2
\end{aligned}
$$

---

# Bias

- So it turns out the intuitive estimator is **not** the unbiased estimator!

  - Note that $S^2 \equiv \frac{1}{n}\sum_i (Y_i - \bar{Y})^2$ is the appropriate formula for calculating the variance of a sample
  
  - But it is **not** the appropriate estimator for calculating the variance of a population!
  
- Although how bad are we messing up if we make this mistake?

$$
\begin{aligned}
B(S^2) &= E[S^2] - \sigma^2 \\
&= \frac{n-1}{n}\sigma^2 - \sigma^2\\
&= \bigg(\frac{n-1}{n} - 1\bigg)\sigma^2\\
&= \frac{-\sigma^2}{n}
\end{aligned}
$$

- As $n \rightarrow \infty$, $B(S^2) \rightarrow 0$

---

# Interval Estimators

- So we've covered two dimensions of the quality of a **point estimate**: bias and variance

- Recall that we also are interested in **interval estimates**: two numbers that capture the true population parameter

- Specifically, an **interval estimator** is:

  1. A rule...
  
  2. ...specifying how we use a sample to calculate numbers...
  
  3. ...that form the endpoints of an interval...
  
  4. ...containing the parameter of interest $\theta$

- We again are interested in the quality of this concept

  - Want it to contain $\theta$
  
  - Want it to be narrow
  
---

# Confidence Intervals

- Interval estimators are commonly called **confidence intervals (CIs)**

- CIs constructed of **upper and lower confidence bounds**

- Probability that CI contains $\theta$ is **the confidence coefficient**

  - The fraction of the time...
  
  - ...in repeated sampling...
  
  - ...that the CI will contain $\theta$
  
- Thus we want the confidence coefficient to be **high**

- Denoted as $1 - \alpha$

- Formally:

$$
P(\hat{\theta}_L \leq \theta \leq \hat{\theta}_H) = (1-\alpha)
$$
---

# Confidence Intervals

- Let's figure out how to construct a CI for a sample statistic $\hat{\theta}$ that is normally distributed with mean $\mu$ and standard error $\sigma_{\hat{\theta}}$

  - Formally, $\hat{\theta} \sim \mathcal{N}(\mu,\sigma_{\hat{\theta}})$
  
- Standardize the statistic as $Z = \frac{\hat{\theta} - \theta}{\sigma_{\hat{\theta}}}$

- To construct the CI, choose two **critical values** $-z_{\alpha/2}$ and $z_{\alpha/2}$ such that

$$
\begin{aligned}
P(-z_{\alpha/2} \leq Z \leq z_{\alpha/2}) &= \int_{-z_{\alpha/2}}^{z_{\alpha/2}} \frac{1}{\sqrt{2\pi}}e^{-t^2/2}dt\\
&= 1-\alpha\\
\end{aligned}
$$

---

# Confidence Intervals

$$
\begin{aligned}
P(-z_{\alpha/2} \leq \frac{\hat{\theta}-\theta}{\sigma_{\hat{\theta}}} \leq z_{\alpha/2}) &= 1-\alpha\\
P(-z_{\alpha/2\sigma_{\hat{\theta}}} \leq \hat{\theta}-\theta \leq z_{\alpha/2\sigma_{\hat{\theta}}}) &= 1-\alpha\\
P(\hat{\theta}-z_{\alpha/2\sigma_{\hat{\theta}}} \leq \theta \leq \hat{\theta}+z_{\alpha/2\sigma_{\hat{\theta}}}) &= 1-\alpha
\end{aligned}
$$

- Thus $\hat{\theta}_L = \hat{\theta}-z_{\alpha/2\sigma_{\hat{\theta}}}$ and $\hat{\theta}_H = \hat{\theta}+z_{\alpha/2\sigma_{\hat{\theta}}}$

- But how do we determine $z_{\alpha/2}$

- CLT to the rescue!

---

# Confidence Intervals

- CLT to the rescue!

- Sampling distribution approximates the normal as $n\rightarrow \infty$

```{r,echo=F,warning=F,message=F}
# Load necessary library
library(ggplot2)

# Define the mean and standard deviation
mean <- 0
sd <- 1

# Define the range for x values
x <- seq(-4, 4, length.out = 1000)

# Define the density function for the normal distribution
y <- dnorm(x, mean, sd)

# Calculate the critical value for a 95% confidence interval
z <- qnorm(c(0.025, 0.975))

# Create the plot
ggplot(data.frame(x, y), aes(x, y)) +
  geom_line() +
  # geom_ribbon(data = subset(data.frame(x, y), x > z[1] & x < z[2]), 
  #             aes(ymin = 0, ymax = y), 
  #             fill = "blue", 
  #             alpha = 0.5) +
  labs(title = bquote("Sampling Distribution of"~ bar(Y)),
       x = bquote(bar(y)),
       y = bquote(f(bar(y)))) +
  theme_minimal() + 
  geom_vline(xintercept = 0,linetype = 'dashed') + 
  annotate(geom = 'label',x = 0,y = -Inf,label = expression(E(bar(y))==mu),vjust = 0,parse = T) + 
  annotate(geom = 'segment',x = -.95,xend = .95,y = .25,yend= .25,
           arrow = arrow(length=unit(0.30,"cm"), ends="both", type = "closed")) + 
  annotate(geom = 'label',x = 0,y = Inf,label = expression(sigma[bar(y)]^2==sigma^2/n),vjust = 5.5,parse = T) + 
  theme(axis.text.x = element_blank(),
        axis.title.x = element_text(hjust = 1))
```

---

# Confidence Intervals

- CLT to the rescue!

- Standardized sampling distribution simplifies!

```{r,echo=F,warning=F,message=F}
ggplot(data.frame(x, y), aes(x, y)) +
  geom_line() +
  # geom_ribbon(data = subset(data.frame(x, y), x > z[1] & x < z[2]), 
  #             aes(ymin = 0, ymax = y), 
  #             fill = "blue", 
  #             alpha = 0.5) +
  labs(title = bquote("Sampling Distribution of"~ Z),
       x = bquote(z),
       y = bquote(f(z)==phi)) +
  theme_minimal() + 
  geom_vline(xintercept = 0,linetype = 'dashed') + 
  annotate(geom = 'label',x = 0,y = -Inf,label = expression(E(z)==0),vjust = 0,parse = T) + 
  annotate(geom = 'segment',x = -.95,xend = .95,y = .25,yend= .25,
           arrow = arrow(length=unit(0.30,"cm"), ends="both", type = "closed")) + 
  annotate(geom = 'label',x = 0,y = Inf,label = expression(sigma[z]^2==1),vjust = 5.5,parse = T) + 
  theme(axis.text.x = element_blank(),
        axis.title.x = element_text(hjust = 1))
```

---

# Confidence Intervals

- CLT to the rescue!

- Define our **confidence** with $1-\alpha$ where $\alpha \in [0,1]$

```{r,echo=F,warning=F,message=F}
ggplot(data.frame(x, y), aes(x, y)) +
  geom_line() +
  geom_ribbon(data = subset(data.frame(x, y), x > z[1] & x < z[2]),
              aes(ymin = 0, ymax = y),
              fill = "blue",
              alpha = 0.5) +
  labs(title = bquote("Sampling Distribution of"~ Z),
       x = bquote(z),
       y = bquote(f(z)==phi)) +
  theme_minimal() + 
  geom_vline(xintercept = 0,linetype = 'dashed') + 
  annotate(geom = 'label',x = 0,y = -Inf,label = expression(E(z)==0),vjust = 0,parse = T) + 
  # annotate(geom = 'segment',x = -.95,xend = .95,y = .25,yend= .25,
  #          arrow = arrow(length=unit(0.30,"cm"), ends="both", type = "closed")) + 
  annotate(geom = 'label',x = 0,y = Inf,label = expression(1-alpha),vjust = 5.5,parse = T) + 
  annotate(geom = 'label',x = -2.5,y = .01,label = expression(alpha/2),vjust = 0,parse = T) + 
  annotate(geom = 'label',x = 2.5,y = .01,label = expression(alpha/2),vjust = 0,parse = T) + 
  scale_x_continuous(breaks = c(-1.96,1.96),labels = c(expression(-z[alpha/2]),expression(z[alpha/2]))) + 
  theme(axis.title.x = element_text(hjust = 1))
```

---

# Confidence Intervals

- CLT to the rescue!

- Then use easy functions (or lookup tables in ye olden days) to get critical values!

```{r,echo=F,warning=F,message=F}
# Define the cumulative distribution function for the normal distribution
y <- pnorm(x, mean, sd)

# Calculate the critical value for a 95% confidence interval
z <- qnorm(c(0.025, 0.975))

# Create the plot
ggplot(data.frame(x, y), aes(x, y)) +
  geom_line() +
  # geom_ribbon(data = subset(data.frame(x, y), x > z[1] & x < z[2]), 
  #             aes(ymin = 0, ymax = y), 
  #             fill = "blue", 
  #             alpha = 0.5) +
  labs(title = bquote("CDF of"~ Z),
       x = bquote(z),
       y = bquote(F(z)==Phi)) +
  theme_minimal()+
  scale_x_continuous(breaks = c(-1.96,1.96),labels = c(expression(-z[alpha/2]==Phi^{-1}~(alpha/2)),expression(z[alpha/2]==-Phi^{-1}~(alpha/2)))) + 
  theme(axis.title.x = element_text(hjust = 1)) + 
  scale_y_continuous(breaks = c(.025,.975),labels = c(expression(Phi~(-z[alpha/2])==alpha/2),expression(Phi~(z[alpha/2])==1-alpha/2))) + 
  annotate(geom = 'segment',x = -4,xend = -4,y = .025,yend= .975,
           arrow = arrow(length=unit(0.30,"cm"), ends="both", type = "closed")) + 
  annotate(geom = 'label',x = -4,y = .5,label = expression(1-alpha),vjust = 0,parse = T) + 
  annotate(geom = 'segment',x = -1.96,y = 0,xend = -1.96,yend = .025,linetype = 'dashed') + 
  annotate(geom = 'segment',x = 1.96,y = 0,xend = 1.96,yend = .975,linetype = 'dashed') + 
  annotate(geom = 'segment',x = -1.96,y = .025,xend = -Inf,yend = .025,linetype = 'dashed') + 
  annotate(geom = 'segment',x = 1.96,y = .975,xend = -Inf,yend = .975,linetype = 'dashed')
```

---

# Critical Values

- The functions to yield $z_{\alpha/2} = -\Phi^{-1}(\frac{\alpha}{2})$ and $-z_{\alpha/2} = \Phi^{-1}(\frac{\alpha}{2})$ are in `R`

- Say I want to calculate a 0.95 CI = $1-\alpha$
```{r}
qnorm(.025)
```

- Say I want to calculate a 0.90 CI = $1-\alpha$
```{r}
qnorm(.05)
```

- Say I want to calculate a 0.99 CI = $1-\alpha$
```{r}
qnorm(.005)
```

---

# Calculating CIs

- So if we want to define the CI for an estimator $\hat{\theta}$ whose sampling distribution is normally distributed, we can write $[\hat{\theta} - 1.96\sigma_{\hat{\theta}},\ \hat{\theta} + 1.96\sigma_{\hat{\theta}}]$

- But what is $\sigma_{\hat{\theta}}$?

  - We know it is $\sqrt{VAR(\hat{\theta})} = \sqrt{\frac{\sigma^2}{n}} = \frac{\sigma}{\sqrt{n}}$
  
- But then what is $\sigma^2$?

- We've been kicking this can down the road for a while now...using CLT to construct CIs for $\mu$, but defined in terms of $\sigma^2$ -- the population standard variance

- Very unusual to know $\sigma^2$ in practice. Homeworks and exercises will tell you, but in practice we don't know

- So we need to approximate $\sigma^2$ using $S^2_U \equiv \frac{\sum_i (Y_i - \bar{Y})^2}{n-1}$, our **unbiased** estimator for $\sigma^2$

---

# Consistency

- But wait! Before we can plug in $S_U$, we need to prove it is both unbiased and **consistent**

- We already know how to prove unbiasedness

- Consistency: as the sample size used to construct the estimator gets large, the probability of it being measured with error gets small

- Denote $\hat{\theta}_n$ as the estimate for a given sample size $n$

  - In the extreme: $\underset{n\rightarrow \infty}\lim P(|\hat{\theta} - \theta| > \epsilon)  = 0$ where $\epsilon$ is any positive number
  
  - Can also express as " $\hat{\theta}_n$ converges in probability to $\theta$ ", or $\hat{\theta}_n \overset{p}{\to} \theta$
  
- In practice, we can evaluate this property by checking whether $VAR(\hat{\theta})$ approaches zero as $n$ gets large (see pg. 450 for proof)

  - $\underset{n\rightarrow \infty}\lim VAR(\hat{\theta}) = 0$

---

# Consistency

- Apply to $\bar{Y}$ for intuition

$$
\begin{aligned}
VAR(\bar{Y}) &= \frac{\sigma^2}{n}\\
\underset{n\rightarrow \infty}\lim \frac{\sigma^2}{n} &= 0
\end{aligned}
$$

- Note that this **by itself** is insufficient to claim $\bar{Y} \overset{p}{\to} \mu$...we need to also prove unbiasedness (which we did last class)

- In other words, an estimator might be **consistent** but **biased**

- Or an estimator might be **unbiased** but not **consistent**

- Need to check both!

---

# $\sigma^2$

- Remember what we're doing here!

  - We know that $U_n \equiv \frac{\bar{Y} - \mu}{\sqrt{\sigma^2/n}} \sim \mathcal{N}(\mu,\sigma^2$)

  - But can we be sure that $\hat{\theta} \equiv \frac{\bar{Y} - \mu}{\sqrt{S^2_U/n}} \sim \mathcal{N}(\mu,\sigma^2)$?
  
- Note that, in the original setting, $\sigma^2$ is a **parameter** whereas in our sample setting $S^2_U$ is a **random variable**

$$
F\bigg(\frac{\bar{Y} - \mu}{S_U / \sqrt{n}}\bigg) \overset{p}{\to} \Phi
$$

---

# $\sigma^2$

- So let's examine whether $S^2_U$ is a **consistent** estimator for $\sigma^2$

$$
\begin{aligned}
S^2_U &= \frac{\sum_i (Y_i - \bar{Y})^2}{n-1}\\
&= \frac{1}{n-1}\bigg(\sum_i Y_i^2 + \sum_i \bar{Y}^2 - \sum_i 2Y_i\bar{Y}\bigg)\\
&= \frac{1}{n-1}\bigg((\sum_i Y_i^2) + n\bar{Y}^2 - 2n\bar{Y}^2\bigg)\\
&= \frac{1}{n-1}\bigg((\sum_i Y_i^2) - n\bar{Y}^2\bigg)\\
&= \frac{n}{n-1}\bigg(\frac{1}{n}\sum_i Y_i^2 - \bar{Y}^2\bigg)\\
\end{aligned}
$$

---

# $\sigma^2$

- So let's examine whether $S^2_U$ is a **consistent** estimator for $\sigma^2$
$$
\begin{aligned}
S^2_U &= \frac{n}{n-1}\bigg(\color{red}{\frac{1}{n}\sum_i Y_i^2 - \bar{Y}^2}\bigg)\\
\underset{n\rightarrow \infty}\lim \frac{1}{n}\sum_i Y_i^2 - \bar{Y}^2 &= \underset{n\rightarrow \infty}\lim \frac{1}{n}\sum_i Y_i^2 - \underset{n\rightarrow \infty}\lim \frac{1}{n}\sum_i\bar{Y}^2\\
&= \mu_{Y^2} - \mu_Y^2\\
&= E[Y^2] - \mu^2\\
&= \sigma^2\\
\text{So:}\ \ S^2_U &= \frac{n}{n-1}(\sigma^2)
\end{aligned}
$$

- But $\underset{n\rightarrow \infty}\lim \frac{n}{n-1} = 1$, meaning $S^2_U \overset{p}{\to} \sigma^2$

```{r,message=F,echo=F,warning=F,results='hide'}
dir <- getwd()
type <- 'pdf'
format <- 'landscape'
f <- 'Lecture_8_slides'

system(paste('Rscript ../NFGH/chromeprint.R',dir,type,format,f),wait = F)
```